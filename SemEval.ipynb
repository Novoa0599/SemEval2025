{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5bfd027-d2cf-48ff-b4f5-1b0f5f0b6869",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b71453c-3b2a-4b3b-acce-473cf44d91ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Anger</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Surprise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eng_train_track2_001</td>\n",
       "      <td>None of us has mentioned the incident since.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eng_train_track2_002</td>\n",
       "      <td>I was 7 and woke up early, so I went to the ba...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eng_train_track2_003</td>\n",
       "      <td>By that point I felt like someone was stabbing...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eng_train_track2_004</td>\n",
       "      <td>watching her leave with dudes drove me crazy.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eng_train_track2_005</td>\n",
       "      <td>`` My eyes widened.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2763</th>\n",
       "      <td>eng_train_track2_2764</td>\n",
       "      <td>My face is cold, and my hands are guilty.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2764</th>\n",
       "      <td>eng_train_track2_2765</td>\n",
       "      <td>I remembered how I dragged his box into the be...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2765</th>\n",
       "      <td>eng_train_track2_2766</td>\n",
       "      <td>As I walked in the door she came around the co...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2766</th>\n",
       "      <td>eng_train_track2_2767</td>\n",
       "      <td>They kept me at the hospital for 24 hours-and ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2767</th>\n",
       "      <td>eng_train_track2_2768</td>\n",
       "      <td>I stopped a couple times to stretch out my cal...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2768 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id  \\\n",
       "0      eng_train_track2_001   \n",
       "1      eng_train_track2_002   \n",
       "2      eng_train_track2_003   \n",
       "3      eng_train_track2_004   \n",
       "4      eng_train_track2_005   \n",
       "...                     ...   \n",
       "2763  eng_train_track2_2764   \n",
       "2764  eng_train_track2_2765   \n",
       "2765  eng_train_track2_2766   \n",
       "2766  eng_train_track2_2767   \n",
       "2767  eng_train_track2_2768   \n",
       "\n",
       "                                                   text  Joy  Fear  Anger  \\\n",
       "0          None of us has mentioned the incident since.    0     1      0   \n",
       "1     I was 7 and woke up early, so I went to the ba...    1     0      0   \n",
       "2     By that point I felt like someone was stabbing...    0     3      0   \n",
       "3         watching her leave with dudes drove me crazy.    0     1      3   \n",
       "4                                   `` My eyes widened.    0     1      0   \n",
       "...                                                 ...  ...   ...    ...   \n",
       "2763          My face is cold, and my hands are guilty.    0     1      0   \n",
       "2764  I remembered how I dragged his box into the be...    1     0      0   \n",
       "2765  As I walked in the door she came around the co...    3     0      0   \n",
       "2766  They kept me at the hospital for 24 hours-and ...    0     1      0   \n",
       "2767  I stopped a couple times to stretch out my cal...    0     0      0   \n",
       "\n",
       "      Sadness  Surprise  \n",
       "0           2         1  \n",
       "1           0         0  \n",
       "2           0         0  \n",
       "3           1         0  \n",
       "4           0         2  \n",
       "...       ...       ...  \n",
       "2763        1         0  \n",
       "2764        0         0  \n",
       "2765        0         1  \n",
       "2766        1         0  \n",
       "2767        0         0  \n",
       "\n",
       "[2768 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar el archivo CSV\n",
    "df = pd.read_csv(\"eng_train.csv\")\n",
    "\n",
    "# Verificar la estructura del archivo\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b0fd16e-ced1-4b33-bb42-f83ec39dfb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "import nltk\n",
    "import unicodedata\n",
    "import requests\n",
    "from spacy_syllables import SpacySyllables\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import TweetTokenizer\n",
    "from spacy.lang.es import Spanish\n",
    "from spacy.lang.en import English\n",
    "from nltk.util import ngrams\n",
    "import pandas as pd\n",
    "import contractions  # Importamos la librería para expandir contracciones\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "\n",
    "#df = pd.DataFrame(df_final)\n",
    "\n",
    "# Clase TextProcessing ya definida anteriormente\n",
    "class TextProcessing(object):\n",
    "    name = 'Text Processing'\n",
    "    lang = 'en'\n",
    "\n",
    "    def __init__(self, lang: str = 'en'):\n",
    "        self.lang = lang\n",
    "\n",
    "    @staticmethod\n",
    "    def nlp(text: str) -> list:\n",
    "        try:\n",
    "            list_tagger = []\n",
    "            tp_nlp = TextProcessing.load_spacy(TextProcessing.lang)\n",
    "            doc = tp_nlp(text.lower())\n",
    "            for token in doc:\n",
    "                item = {'text': token.text, 'lemma': token.lemma_, 'pos': token.pos_, 'tag': token.tag_,\n",
    "                        'dep': token.dep_, 'shape': token.shape_, 'is_alpha': token.is_alpha,\n",
    "                        'is_stop': token.is_stop, 'is_digit': token.is_digit, 'is_punct': token.is_punct,\n",
    "                        'syllables': token._.syllables}\n",
    "                list_tagger.append(item)\n",
    "            return list_tagger\n",
    "        except Exception as e:\n",
    "            print('Error nlp: {0}'.format(e))\n",
    "\n",
    "    @staticmethod\n",
    "    def load_spacy(lang: str) -> object:\n",
    "        try:\n",
    "            spacy_model = {'es': 'es_core_news_sm', 'en': 'en_core_web_sm'}\n",
    "            if not spacy.util.is_package(spacy_model[lang]):\n",
    "                spacy.cli.download(spacy_model[lang])\n",
    "\n",
    "            component = spacy.load(spacy_model[lang])\n",
    "            SpacySyllables(component)\n",
    "            component.add_pipe('syllables', last=True)\n",
    "            return component\n",
    "        except Exception as e:\n",
    "            print('Error load spacy: {0}'.format(e))\n",
    "\n",
    "    @staticmethod\n",
    "    def proper_encoding(text: str) -> str:\n",
    "        try:\n",
    "            text = unicodedata.normalize('NFD', text)\n",
    "            text = text.encode('ascii', 'ignore')\n",
    "            return text.decode(\"utf-8\")\n",
    "        except Exception as e:\n",
    "            print('Error proper_encoding: {0}'.format(e))\n",
    "\n",
    "    @staticmethod\n",
    "    def stopwords(text: str) -> str:\n",
    "        try:\n",
    "            nlp = English()\n",
    "            doc = nlp(text)\n",
    "            token_list = [token.text for token in doc]\n",
    "            sentence = []\n",
    "            for word in token_list:\n",
    "                lexeme = nlp.vocab[word]\n",
    "                if not lexeme.is_stop:\n",
    "                    sentence.append(word)\n",
    "            return ' '.join(sentence)\n",
    "        except Exception as e:\n",
    "            print('Error stopwords: {0}'.format(e))\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_patterns(text: str) -> str:\n",
    "        try:\n",
    "            text = re.sub(r'\\©|\\×|\\⇔|\\_|\\»|\\«|\\~|\\#|\\$|\\€|\\Â|\\�|\\¬', '', text)\n",
    "            text = re.sub(r'\\,|\\;|\\:|\\!|\\¡|\\’|\\‘|\\”|\\“|\\\"|\\'|\\`', '', text)\n",
    "            text = re.sub(r'\\}|\\{|\\[|\\]|\\(|\\)|\\<|\\>|\\?|\\¿|\\°|\\|', '', text)\n",
    "            text = re.sub(r'\\/|\\-|\\+|\\*|\\=|\\^|\\%|\\&|\\$', '', text)\n",
    "            text = re.sub(r'\\b\\d+(?:\\.\\d+)?\\s+', '', text)\n",
    "            return text.lower()\n",
    "        except Exception as e:\n",
    "            print('Error remove_patterns: {0}'.format(e))\n",
    "\n",
    "    @staticmethod\n",
    "    def expand_contractions(text: str) -> str:\n",
    "        \"\"\"Expande las contracciones en el texto.\"\"\"\n",
    "        try:\n",
    "            return contractions.fix(text)\n",
    "        except Exception as e:\n",
    "            print('Error expand_contractions: {0}'.format(e))\n",
    "\n",
    "    @staticmethod\n",
    "    def transformer(text: str, stopwords: bool = False) -> str:\n",
    "        try:\n",
    "            text_out = TextProcessing.proper_encoding(text)\n",
    "            text_out = TextProcessing.expand_contractions(text_out)  # Expandimos las contracciones\n",
    "            text_out = text_out.lower()\n",
    "            text_out = re.sub(\"[\\U0001f000-\\U000e007f]\", '[EMOJI]', text_out)\n",
    "            text_out = re.sub(\n",
    "                r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+'\n",
    "                r'|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))',\n",
    "                '[URL]', text_out)\n",
    "            text_out = re.sub(\"@\", '[MENTION]', text_out)\n",
    "            text_out = re.sub(\"#([A-Za-z0-9_]{1,40})\", '[HASTAG]', text_out)\n",
    "            text_out = TextProcessing.remove_patterns(text_out)\n",
    "            text_out = TextProcessing.stopwords(text_out) if stopwords else text_out\n",
    "            text_out = re.sub(r'\\s+', ' ', text_out).strip()\n",
    "            text_out = text_out.rstrip()\n",
    "            return text_out if text_out != ' ' else None\n",
    "        except Exception as e:\n",
    "            print('Error transformer: {0}'.format(e))\n",
    "\n",
    "\n",
    "# Función para aplicar el preprocesamiento al DataFrame\n",
    "def apply_preprocessing_to_df(df: pd.DataFrame, column: str) -> pd.DataFrame:\n",
    "    tp = TextProcessing(lang='en')  # Inicializamos el preprocesador en inglés\n",
    "    df[f'{column}_processed'] = df[column].apply(lambda x: tp.transformer(x))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25bb28be-d851-4aee-836b-e16ef4055a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         id  \\\n",
      "2124  eng_train_track2_2125   \n",
      "2716  eng_train_track2_2717   \n",
      "2232  eng_train_track2_2233   \n",
      "261    eng_train_track2_262   \n",
      "2059  eng_train_track2_2060   \n",
      "\n",
      "                                                   text  Joy  Fear  Anger  \\\n",
      "2124  26 January 2011 @ 04:45 pm Boys & Girls 718 Cr...    0     2      0   \n",
      "2716  We headed north on a sunny Saturday morning in...    1     0      0   \n",
      "2232  I looked down to find five small white arrows ...    0     3      0   \n",
      "261                         I've never gone back there.    0     2      0   \n",
      "2059                                     My heart sank.    0     0      0   \n",
      "\n",
      "      Sadness  Surprise      V      A      D  \\\n",
      "2124        0         0  0.730  0.840  0.293   \n",
      "2716        0         0  0.980  0.824  0.794   \n",
      "2232        3         2  0.688  0.642  0.312   \n",
      "261         2         0  0.625  0.564  0.228   \n",
      "2059        2         0  0.520  0.288  0.164   \n",
      "\n",
      "                                         text_processed  \n",
      "2124  january mention pm boys girls crawled out of b...  \n",
      "2716  we headed north on a sunny saturday morning in...  \n",
      "2232  i looked down to find five small white arrows ...  \n",
      "261                       i have never gone back there.  \n",
      "2059                                     my heart sank.  \n",
      "                         id  \\\n",
      "1378  eng_train_track2_1379   \n",
      "839    eng_train_track2_840   \n",
      "2164  eng_train_track2_2165   \n",
      "2619  eng_train_track2_2620   \n",
      "927    eng_train_track2_928   \n",
      "\n",
      "                                                   text  Joy  Fear  Anger  \\\n",
      "1378                       Colorado, middle of nowhere.    0     1      0   \n",
      "839   This involved swimming a pretty large lake tha...    0     2      0   \n",
      "2164        It was one of my most shameful experiences.    0     1      0   \n",
      "2619  After all, I had vegetables coming out my ears...    0     0      0   \n",
      "927                         Then the screaming started.    0     3      0   \n",
      "\n",
      "      Sadness  Surprise      V      A      D  \\\n",
      "1378        0         1  0.802  0.857  0.427   \n",
      "839         0         0  0.730  0.840  0.293   \n",
      "2164        3         0  0.573  0.426  0.196   \n",
      "2619        0         0  0.500  0.500  0.500   \n",
      "927         1         2  0.743  0.760  0.361   \n",
      "\n",
      "                                         text_processed  \n",
      "1378                        colorado middle of nowhere.  \n",
      "839   this involved swimming a pretty large lake tha...  \n",
      "2164        it was one of my most shameful experiences.  \n",
      "2619  after all i had vegetables coming out my ears ...  \n",
      "927                         then the screaming started.  \n"
     ]
    }
   ],
   "source": [
    "#Emotion weights in VAD\n",
    "vad_values2 = {\n",
    "    \"Anger\": {\"V\": 0.167, \"A\": 0.865, \"D\": 0.657},\n",
    "    \"Fear\": {\"V\": 0.73, \"A\": 0.840, \"D\": 0.293},\n",
    "    \"Joy\": {\"V\": 0.980, \"A\": 0.824, \"D\": 0.794},\n",
    "    \"Sadness\": {\"V\": 0.52, \"A\": 0.288, \"D\": 0.164},\n",
    "    \"Surprise\": {\"V\": 0.875, \"A\": 0.875, \"D\": 0.562},\n",
    "}\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# Función para calcular V, A y D\n",
    "def calculate_vad(row):\n",
    "    emotions = [\"Anger\", \"Fear\", \"Joy\", \"Sadness\", \"Surprise\"]\n",
    "    total_intensity = sum(row[emotion] for emotion in emotions)\n",
    "    if total_intensity == 0:\n",
    "        return 0.500, 0.500, 0.500  # Valores para \"No Emotion\"\n",
    "\n",
    "    V = sum(row[emotion] * vad_values2[emotion][\"V\"] for emotion in emotions) / total_intensity\n",
    "    A = sum(row[emotion] * vad_values2[emotion][\"A\"] for emotion in emotions) / total_intensity\n",
    "    D = sum(row[emotion] * vad_values2[emotion][\"D\"] for emotion in emotions) / total_intensity\n",
    "    return round(V, 3), round(A, 3), round(D, 3)\n",
    "\n",
    "# Aplicar la función para calcular V, A y D\n",
    "df[[\"V\", \"A\", \"D\"]] = df.apply(calculate_vad, axis=1, result_type=\"expand\")\n",
    "\n",
    "# Preprocesar texto\n",
    "tp = TextProcessing(lang='en')\n",
    "df[\"text_processed\"] = df[\"text\"].apply(lambda x: tp.transformer(x))\n",
    "\n",
    "# Dividir los datos en entrenamiento y validación (80-20)\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(train_df.head())\n",
    "print(val_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f504406-fc98-489e-9f3e-f8c938ce7e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 22:48:38.440791: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-27 22:48:38.452031: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740696518.464895  214662 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740696518.468769  214662 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-27 22:48:38.483712: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/opt/jupyterhub/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2690: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_scheduler\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import unicodedata\n",
    "from spacy_syllables import SpacySyllables\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.functional import softmax\n",
    "# Tokenización con BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "def BERT_tokenization(df, text_column):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for sent in df[text_column]:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            sent,\n",
    "            add_special_tokens=True,\n",
    "            max_length=128,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    return torch.cat(input_ids, dim=0), torch.cat(attention_masks, dim=0)\n",
    "\n",
    "train_input_ids, train_attention_masks = BERT_tokenization(train_df, \"text_processed\")\n",
    "val_input_ids, val_attention_masks = BERT_tokenization(val_df, \"text_processed\")\n",
    "\n",
    "# Convertir etiquetas\n",
    "train_labels = torch.tensor(train_df[[\"V\", \"A\", \"D\"]].values, dtype=torch.float32)\n",
    "val_labels = torch.tensor(val_df[[\"V\", \"A\", \"D\"]].values, dtype=torch.float32)\n",
    "\n",
    "# Crear datasets\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "val_dataset = TensorDataset(val_input_ids, val_attention_masks, val_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ed34354-10a8-410d-a8c7-218286a0582b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/jupyterhub/lib/python3.12/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 277/277 [14:08<00:00,  3.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.6157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 70/70 [01:04<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6498\n",
      "Precision (Micro): 0.6498, Precision (Macro): 0.6571\n",
      "Recall (Micro): 0.6498, Recall (Macro): 0.6587\n",
      "F1 (Micro): 0.6498, F1 (Macro): 0.6497\n",
      "Pearson r (V): 0.5287\n",
      "Pearson r (A): 0.5386\n",
      "Pearson r (D): 0.6569\n",
      "\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 277/277 [14:17<00:00,  3.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.5822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 70/70 [01:04<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7455\n",
      "Precision (Micro): 0.7455, Precision (Macro): 0.7476\n",
      "Recall (Micro): 0.7455, Recall (Macro): 0.7519\n",
      "F1 (Micro): 0.7455, F1 (Macro): 0.7448\n",
      "Pearson r (V): 0.5405\n",
      "Pearson r (A): 0.5868\n",
      "Pearson r (D): 0.6818\n",
      "\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 277/277 [14:08<00:00,  3.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.5663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 70/70 [01:04<00:00,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7437\n",
      "Precision (Micro): 0.7437, Precision (Macro): 0.7494\n",
      "Recall (Micro): 0.7437, Recall (Macro): 0.7528\n",
      "F1 (Micro): 0.7437, F1 (Macro): 0.7434\n",
      "Pearson r (V): 0.5489\n",
      "Pearson r (A): 0.5902\n",
      "Pearson r (D): 0.6759\n",
      "\n",
      "Final Average Metrics Across Epochs:\n",
      "Accuracy: 0.7130\n",
      "Precision (Micro): 0.7130, Precision (Macro): 0.7181\n",
      "Recall (Micro): 0.7130, Recall (Macro): 0.7211\n",
      "F1 (Micro): 0.7130, F1 (Macro): 0.7126\n",
      "Pearson r (V): 0.5394\n",
      "Pearson r (A): 0.5719\n",
      "Pearson r (D): 0.6715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#TRAIN\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from scipy.stats import pearsonr  # Para calcular Pearson r\n",
    "from tqdm import tqdm\n",
    "from transformers import BertForSequenceClassification, AdamW, get_scheduler\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "# Nota: Asegúrate de que train_dataset y val_dataset estén correctamente definidos.\n",
    "train_dataloader  = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size)\n",
    "validation_dataloader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=batch_size)\n",
    "\n",
    "# Configurar el modelo BERT\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=3,  # Tres etiquetas: V, A, D\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# Configurar optimizador y scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "num_epochs = 3\n",
    "num_training_steps = len(train_dataloader) * num_epochs\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "def evaluate_metrics(predictions, labels):\n",
    "    \"\"\"\n",
    "    Calcula métricas de clasificación generales (a partir del argmax)\n",
    "    y calcula el Pearson r para cada dimensión (V, A y D) individualmente.\n",
    "    \"\"\"\n",
    "    # Para las métricas de clasificación usamos argmax\n",
    "    pred_flat = np.argmax(predictions, axis=1)\n",
    "    labels_flat = np.argmax(labels, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(labels_flat, pred_flat)\n",
    "    precision_micro = precision_score(labels_flat, pred_flat, average=\"micro\")\n",
    "    precision_macro = precision_score(labels_flat, pred_flat, average=\"macro\")\n",
    "    recall_micro = recall_score(labels_flat, pred_flat, average=\"micro\")\n",
    "    recall_macro = recall_score(labels_flat, pred_flat, average=\"macro\")\n",
    "    f1_micro = f1_score(labels_flat, pred_flat, average=\"micro\")\n",
    "    f1_macro = f1_score(labels_flat, pred_flat, average=\"macro\")\n",
    "    \n",
    "    # Ahora, para Pearson r, se calcula por cada emoción (cada columna)\n",
    "    # Se asume que las dimensiones son: índice 0 = V, 1 = A y 2 = D.\n",
    "    emotion_names = ['V', 'A', 'D']\n",
    "    pearson_r_dict = {}\n",
    "    for i in range(predictions.shape[1]):\n",
    "        pred_i = predictions[:, i]\n",
    "        label_i = labels[:, i]\n",
    "        if np.std(pred_i) == 0 or np.std(label_i) == 0:\n",
    "            pearson_r_i = float('nan')\n",
    "        else:\n",
    "            pearson_r_i, _ = pearsonr(pred_i, label_i)\n",
    "        pearson_r_dict[emotion_names[i]] = pearson_r_i\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision_micro\": precision_micro,\n",
    "        \"precision_macro\": precision_macro,\n",
    "        \"recall_micro\": recall_micro,\n",
    "        \"recall_macro\": recall_macro,\n",
    "        \"f1_micro\": f1_micro,\n",
    "        \"f1_macro\": f1_macro,\n",
    "        \"pearson_r\": pearson_r_dict\n",
    "    }\n",
    "\n",
    "def train_and_evaluate():\n",
    "    metrics_per_epoch = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "        \n",
    "        # Fase de entrenamiento\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in tqdm(train_dataloader, desc=\"Training\"):\n",
    "            b_input_ids, b_attention_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "            model.zero_grad()\n",
    "\n",
    "            outputs = model(b_input_ids, attention_mask=b_attention_mask, labels=b_labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Fase de evaluación\n",
    "        model.eval()\n",
    "        eval_predictions = []\n",
    "        eval_labels = []\n",
    "\n",
    "        for batch in tqdm(validation_dataloader, desc=\"Validation\"):\n",
    "            b_input_ids, b_attention_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(b_input_ids, attention_mask=b_attention_mask)\n",
    "                logits = outputs.logits\n",
    "\n",
    "            eval_predictions.extend(logits.detach().cpu().numpy())\n",
    "            eval_labels.extend(b_labels.detach().cpu().numpy())\n",
    "\n",
    "        metrics = evaluate_metrics(np.array(eval_predictions), np.array(eval_labels))\n",
    "        metrics[\"train_loss\"] = avg_train_loss\n",
    "        metrics_per_epoch.append(metrics)\n",
    "        \n",
    "        print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        print(f\"Precision (Micro): {metrics['precision_micro']:.4f}, Precision (Macro): {metrics['precision_macro']:.4f}\")\n",
    "        print(f\"Recall (Micro): {metrics['recall_micro']:.4f}, Recall (Macro): {metrics['recall_macro']:.4f}\")\n",
    "        print(f\"F1 (Micro): {metrics['f1_micro']:.4f}, F1 (Macro): {metrics['f1_macro']:.4f}\")\n",
    "        # Imprimir Pearson r para cada emoción\n",
    "        for emo, pr in metrics['pearson_r'].items():\n",
    "            print(f\"Pearson r ({emo}): {pr:.4f}\")\n",
    "\n",
    "    # Calcular métricas promedio a lo largo de las épocas\n",
    "    avg_metrics = {}\n",
    "    for key in metrics_per_epoch[0].keys():\n",
    "        if key != 'pearson_r':\n",
    "            avg_metrics[key] = np.mean([epoch_metrics[key] for epoch_metrics in metrics_per_epoch])\n",
    "        else:\n",
    "            avg_metrics[key] = {}\n",
    "            for emotion in metrics_per_epoch[0]['pearson_r'].keys():\n",
    "                avg_metrics[key][emotion] = np.mean([epoch_metrics['pearson_r'][emotion] for epoch_metrics in metrics_per_epoch])\n",
    "\n",
    "    print(\"\\nFinal Average Metrics Across Epochs:\")\n",
    "    print(f\"Accuracy: {avg_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision (Micro): {avg_metrics['precision_micro']:.4f}, Precision (Macro): {avg_metrics['precision_macro']:.4f}\")\n",
    "    print(f\"Recall (Micro): {avg_metrics['recall_micro']:.4f}, Recall (Macro): {avg_metrics['recall_macro']:.4f}\")\n",
    "    print(f\"F1 (Micro): {avg_metrics['f1_micro']:.4f}, F1 (Macro): {avg_metrics['f1_macro']:.4f}\")\n",
    "    for emo, pr in avg_metrics['pearson_r'].items():\n",
    "        print(f\"Pearson r ({emo}): {pr:.4f}\")\n",
    "\n",
    "    return metrics_per_epoch, avg_metrics\n",
    "\n",
    "# Ejecutar el entrenamiento y la evaluación\n",
    "metrics_per_epoch, avg_metrics = train_and_evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b0e4abe-a49f-4dcc-8f09-78267855c93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo y tokenizador guardados en ./modelo_entrenado_completo\n"
     ]
    }
   ],
   "source": [
    "#save the model\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "# Directorio donde se guardará el modelo\n",
    "output_dir = \"./modelo_entrenado_completo\"\n",
    "\n",
    "# Crear el directorio si no existe\n",
    "import os\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Guardar el modelo\n",
    "model.save_pretrained(output_dir)\n",
    "\n",
    "# Guardar el tokenizador\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"Modelo y tokenizador guardados en {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e059d29f-47dc-4964-806d-a169a0802738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), 'modelo_entrenado.pth')\n",
    "print(\"Modelo guardado exitosamente.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63fb8fb7-fb01-4d53-8982-b1c834703094",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 70/70 [01:04<00:00,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Metrics:\n",
      "Accuracy: 0.7437\n",
      "Precision (Micro): 0.7437, Precision (Macro): 0.7494\n",
      "Recall (Micro): 0.7437, Recall (Macro): 0.7528\n",
      "F1 (Micro): 0.7437, F1 (Macro): 0.7434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "def validate_model():\n",
    "    model.eval()  # Cambiar el modelo al modo de evaluación\n",
    "    eval_predictions = []\n",
    "    eval_labels = []\n",
    "\n",
    "    # Proceso de validación\n",
    "    for batch in tqdm(validation_dataloader, desc=\"Validating\"):\n",
    "        b_input_ids, b_attention_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, attention_mask=b_attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "        eval_predictions.extend(logits.detach().cpu().numpy())\n",
    "        eval_labels.extend(b_labels.detach().cpu().numpy())\n",
    "\n",
    "    # Calcular métricas\n",
    "    metrics = evaluate_metrics(np.array(eval_predictions), np.array(eval_labels))\n",
    "    \n",
    "    print(\"\\nValidation Metrics:\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision (Micro): {metrics['precision_micro']:.4f}, Precision (Macro): {metrics['precision_macro']:.4f}\")\n",
    "    print(f\"Recall (Micro): {metrics['recall_micro']:.4f}, Recall (Macro): {metrics['recall_macro']:.4f}\")\n",
    "    print(f\"F1 (Micro): {metrics['f1_micro']:.4f}, F1 (Macro): {metrics['f1_macro']:.4f}\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# Ejecutar la validación\n",
    "validation_metrics = validate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2ccfc5-ff55-44af-8e8f-c265e34b9678",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03cfc34e-1217-4f8e-bd64-cd541e01ae40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eng_dev_track2_001</td>\n",
       "      <td>I have a floor shift in the morning, hopefully...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eng_dev_track2_002</td>\n",
       "      <td>What is it about this winter that is making me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eng_dev_track2_003</td>\n",
       "      <td>Longest, most awkward drive I've ever taken.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eng_dev_track2_004</td>\n",
       "      <td>I know not why, I wipe my face.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eng_dev_track2_005</td>\n",
       "      <td>And I laughed like this: garhahagar, because m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>eng_dev_track2_112</td>\n",
       "      <td>My heart sank.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>eng_dev_track2_113</td>\n",
       "      <td>I remember the sweat burning in my eyes and tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>eng_dev_track2_114</td>\n",
       "      <td>My sister was walking backwards and bumped her...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>eng_dev_track2_115</td>\n",
       "      <td>I can't breathe right, my head has been stuffe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>eng_dev_track2_116</td>\n",
       "      <td>I had to chord with my thumb.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>116 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                               text\n",
       "0    eng_dev_track2_001  I have a floor shift in the morning, hopefully...\n",
       "1    eng_dev_track2_002  What is it about this winter that is making me...\n",
       "2    eng_dev_track2_003       Longest, most awkward drive I've ever taken.\n",
       "3    eng_dev_track2_004                    I know not why, I wipe my face.\n",
       "4    eng_dev_track2_005  And I laughed like this: garhahagar, because m...\n",
       "..                  ...                                                ...\n",
       "111  eng_dev_track2_112                                     My heart sank.\n",
       "112  eng_dev_track2_113  I remember the sweat burning in my eyes and tr...\n",
       "113  eng_dev_track2_114  My sister was walking backwards and bumped her...\n",
       "114  eng_dev_track2_115  I can't breathe right, my head has been stuffe...\n",
       "115  eng_dev_track2_116                      I had to chord with my thumb.\n",
       "\n",
       "[116 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Cargar los datos del archivo CSV\n",
    "df1 = pd.read_csv(\"eng_dev.csv\")\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac194c11-ddae-47b1-b5b6-dc1713d0b409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   id                                               text  \\\n",
      "0  eng_dev_track2_001  I have a floor shift in the morning, hopefully...   \n",
      "1  eng_dev_track2_002  What is it about this winter that is making me...   \n",
      "2  eng_dev_track2_003       Longest, most awkward drive I've ever taken.   \n",
      "3  eng_dev_track2_004                    I know not why, I wipe my face.   \n",
      "4  eng_dev_track2_005  And I laughed like this: garhahagar, because m...   \n",
      "\n",
      "                                     text_processed1  \n",
      "0  i have a floor shift in the morning hopefully ...  \n",
      "1  what is it about this winter that is making me...  \n",
      "2      longest most awkward drive i have ever taken.  \n",
      "3                     i know not why i wipe my face.  \n",
      "4  and i laughed like this garhahagar because my ...  \n"
     ]
    }
   ],
   "source": [
    "#DATA TO VALIDATE DEV (revisar)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Cargar los datos del archivo CSV\n",
    "df1 = pd.read_csv(\"eng_dev.csv\")\n",
    "# Clase TextProcessing ya definida anteriormente\n",
    "class TextProcessing(object):\n",
    "    name = 'Text Processing'\n",
    "    lang = 'en'\n",
    "\n",
    "    def __init__(self, lang: str = 'en'):\n",
    "        self.lang = lang\n",
    "\n",
    "    @staticmethod\n",
    "    def nlp(text: str) -> list:\n",
    "        try:\n",
    "            list_tagger = []\n",
    "            tp_nlp = TextProcessing.load_spacy(TextProcessing.lang)\n",
    "            doc = tp_nlp(text.lower())\n",
    "            for token in doc:\n",
    "                item = {'text': token.text, 'lemma': token.lemma_, 'pos': token.pos_, 'tag': token.tag_,\n",
    "                        'dep': token.dep_, 'shape': token.shape_, 'is_alpha': token.is_alpha,\n",
    "                        'is_stop': token.is_stop, 'is_digit': token.is_digit, 'is_punct': token.is_punct,\n",
    "                        'syllables': token._.syllables}\n",
    "                list_tagger.append(item)\n",
    "            return list_tagger\n",
    "        except Exception as e:\n",
    "            print('Error nlp: {0}'.format(e))\n",
    "\n",
    "    @staticmethod\n",
    "    def load_spacy(lang: str) -> object:\n",
    "        try:\n",
    "            spacy_model = {'es': 'es_core_news_sm', 'en': 'en_core_web_sm'}\n",
    "            if not spacy.util.is_package(spacy_model[lang]):\n",
    "                spacy.cli.download(spacy_model[lang])\n",
    "\n",
    "            component = spacy.load(spacy_model[lang])\n",
    "            SpacySyllables(component)\n",
    "            component.add_pipe('syllables', last=True)\n",
    "            return component\n",
    "        except Exception as e:\n",
    "            print('Error load spacy: {0}'.format(e))\n",
    "\n",
    "    @staticmethod\n",
    "    def proper_encoding(text: str) -> str:\n",
    "        try:\n",
    "            text = unicodedata.normalize('NFD', text)\n",
    "            text = text.encode('ascii', 'ignore')\n",
    "            return text.decode(\"utf-8\")\n",
    "        except Exception as e:\n",
    "            print('Error proper_encoding: {0}'.format(e))\n",
    "\n",
    "    @staticmethod\n",
    "    def stopwords(text: str) -> str:\n",
    "        try:\n",
    "            nlp = English()\n",
    "            doc = nlp(text)\n",
    "            token_list = [token.text for token in doc]\n",
    "            sentence = []\n",
    "            for word in token_list:\n",
    "                lexeme = nlp.vocab[word]\n",
    "                if not lexeme.is_stop:\n",
    "                    sentence.append(word)\n",
    "            return ' '.join(sentence)\n",
    "        except Exception as e:\n",
    "            print('Error stopwords: {0}'.format(e))\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_patterns(text: str) -> str:\n",
    "        try:\n",
    "            text = re.sub(r'\\©|\\×|\\⇔|\\_|\\»|\\«|\\~|\\#|\\$|\\€|\\Â|\\�|\\¬', '', text)\n",
    "            text = re.sub(r'\\,|\\;|\\:|\\!|\\¡|\\’|\\‘|\\”|\\“|\\\"|\\'|\\`', '', text)\n",
    "            text = re.sub(r'\\}|\\{|\\[|\\]|\\(|\\)|\\<|\\>|\\?|\\¿|\\°|\\|', '', text)\n",
    "            text = re.sub(r'\\/|\\-|\\+|\\*|\\=|\\^|\\%|\\&|\\$', '', text)\n",
    "            text = re.sub(r'\\b\\d+(?:\\.\\d+)?\\s+', '', text)\n",
    "            return text.lower()\n",
    "        except Exception as e:\n",
    "            print('Error remove_patterns: {0}'.format(e))\n",
    "\n",
    "    @staticmethod\n",
    "    def expand_contractions(text: str) -> str:\n",
    "        \"\"\"Expande las contracciones en el texto.\"\"\"\n",
    "        try:\n",
    "            return contractions.fix(text)\n",
    "        except Exception as e:\n",
    "            print('Error expand_contractions: {0}'.format(e))\n",
    "\n",
    "    @staticmethod\n",
    "    def transformer(text: str, stopwords: bool = False) -> str:\n",
    "        try:\n",
    "            text_out = TextProcessing.proper_encoding(text)\n",
    "            text_out = TextProcessing.expand_contractions(text_out)  # Expandimos las contracciones\n",
    "            text_out = text_out.lower()\n",
    "            text_out = re.sub(\"[\\U0001f000-\\U000e007f]\", '[EMOJI]', text_out)\n",
    "            text_out = re.sub(\n",
    "                r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+'\n",
    "                r'|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))',\n",
    "                '[URL]', text_out)\n",
    "            text_out = re.sub(\"@\", '[MENTION]', text_out)\n",
    "            text_out = re.sub(\"#([A-Za-z0-9_]{1,40})\", '[HASTAG]', text_out)\n",
    "            text_out = TextProcessing.remove_patterns(text_out)\n",
    "            text_out = TextProcessing.stopwords(text_out) if stopwords else text_out\n",
    "            text_out = re.sub(r'\\s+', ' ', text_out).strip()\n",
    "            text_out = text_out.rstrip()\n",
    "            return text_out if text_out != ' ' else None\n",
    "        except Exception as e:\n",
    "            print('Error transformer: {0}'.format(e))\n",
    "\n",
    "# Función para aplicar el preprocesamiento al DataFrame\n",
    "def apply_preprocessing_to_df1(df1: pd.DataFrame, column: str) -> pd.DataFrame:\n",
    "    tp = TextProcessing(lang='en')  # Inicializamos el preprocesador en inglés\n",
    "    df1[f'{column}_processed1'] = df1[column].apply(lambda x: tp.transformer(x))\n",
    "    return df1\n",
    "df1 = apply_preprocessing_to_df1(df1, \"text\")\n",
    "print(df1.head()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1e8030e-f653-4233-83dd-d70f32d29e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/jupyterhub/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2690: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Predicting: 100%|██████████| 15/15 [00:13<00:00,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo 'predicciones_decodificadas.csv' con intensidades decodificadas generado exitosamente.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>text_processed1</th>\n",
       "      <th>V_pred</th>\n",
       "      <th>A_pred</th>\n",
       "      <th>D_pred</th>\n",
       "      <th>Anger</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Surprise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eng_dev_track2_001</td>\n",
       "      <td>I have a floor shift in the morning, hopefully...</td>\n",
       "      <td>i have a floor shift in the morning hopefully ...</td>\n",
       "      <td>1.090966</td>\n",
       "      <td>0.930460</td>\n",
       "      <td>-0.251571</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eng_dev_track2_002</td>\n",
       "      <td>What is it about this winter that is making me...</td>\n",
       "      <td>what is it about this winter that is making me...</td>\n",
       "      <td>0.451516</td>\n",
       "      <td>0.554396</td>\n",
       "      <td>-1.276996</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eng_dev_track2_003</td>\n",
       "      <td>Longest, most awkward drive I've ever taken.</td>\n",
       "      <td>longest most awkward drive i have ever taken.</td>\n",
       "      <td>0.565344</td>\n",
       "      <td>0.229077</td>\n",
       "      <td>-1.043907</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eng_dev_track2_004</td>\n",
       "      <td>I know not why, I wipe my face.</td>\n",
       "      <td>i know not why i wipe my face.</td>\n",
       "      <td>0.193684</td>\n",
       "      <td>0.084650</td>\n",
       "      <td>-0.915383</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eng_dev_track2_005</td>\n",
       "      <td>And I laughed like this: garhahagar, because m...</td>\n",
       "      <td>and i laughed like this garhahagar because my ...</td>\n",
       "      <td>1.556907</td>\n",
       "      <td>1.718913</td>\n",
       "      <td>0.196447</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>eng_dev_track2_112</td>\n",
       "      <td>My heart sank.</td>\n",
       "      <td>my heart sank.</td>\n",
       "      <td>0.292723</td>\n",
       "      <td>-0.482987</td>\n",
       "      <td>-1.416687</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>eng_dev_track2_113</td>\n",
       "      <td>I remember the sweat burning in my eyes and tr...</td>\n",
       "      <td>i remember the sweat burning in my eyes and tr...</td>\n",
       "      <td>0.845790</td>\n",
       "      <td>0.871266</td>\n",
       "      <td>-1.125727</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>eng_dev_track2_114</td>\n",
       "      <td>My sister was walking backwards and bumped her...</td>\n",
       "      <td>my sister was walking backwards and bumped her...</td>\n",
       "      <td>0.638080</td>\n",
       "      <td>1.323399</td>\n",
       "      <td>-0.492238</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>eng_dev_track2_115</td>\n",
       "      <td>I can't breathe right, my head has been stuffe...</td>\n",
       "      <td>i cannot breathe right my head has been stuffe...</td>\n",
       "      <td>0.466825</td>\n",
       "      <td>0.062405</td>\n",
       "      <td>-1.392075</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>eng_dev_track2_116</td>\n",
       "      <td>I had to chord with my thumb.</td>\n",
       "      <td>i had to chord with my thumb.</td>\n",
       "      <td>0.566307</td>\n",
       "      <td>1.171798</td>\n",
       "      <td>-0.181457</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>116 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                               text  \\\n",
       "0    eng_dev_track2_001  I have a floor shift in the morning, hopefully...   \n",
       "1    eng_dev_track2_002  What is it about this winter that is making me...   \n",
       "2    eng_dev_track2_003       Longest, most awkward drive I've ever taken.   \n",
       "3    eng_dev_track2_004                    I know not why, I wipe my face.   \n",
       "4    eng_dev_track2_005  And I laughed like this: garhahagar, because m...   \n",
       "..                  ...                                                ...   \n",
       "111  eng_dev_track2_112                                     My heart sank.   \n",
       "112  eng_dev_track2_113  I remember the sweat burning in my eyes and tr...   \n",
       "113  eng_dev_track2_114  My sister was walking backwards and bumped her...   \n",
       "114  eng_dev_track2_115  I can't breathe right, my head has been stuffe...   \n",
       "115  eng_dev_track2_116                      I had to chord with my thumb.   \n",
       "\n",
       "                                       text_processed1    V_pred    A_pred  \\\n",
       "0    i have a floor shift in the morning hopefully ...  1.090966  0.930460   \n",
       "1    what is it about this winter that is making me...  0.451516  0.554396   \n",
       "2        longest most awkward drive i have ever taken.  0.565344  0.229077   \n",
       "3                       i know not why i wipe my face.  0.193684  0.084650   \n",
       "4    and i laughed like this garhahagar because my ...  1.556907  1.718913   \n",
       "..                                                 ...       ...       ...   \n",
       "111                                     my heart sank.  0.292723 -0.482987   \n",
       "112  i remember the sweat burning in my eyes and tr...  0.845790  0.871266   \n",
       "113  my sister was walking backwards and bumped her...  0.638080  1.323399   \n",
       "114  i cannot breathe right my head has been stuffe...  0.466825  0.062405   \n",
       "115                      i had to chord with my thumb.  0.566307  1.171798   \n",
       "\n",
       "       D_pred  Anger  Fear  Joy  Sadness  Surprise  \n",
       "0   -0.251571      0     1    0        1         1  \n",
       "1   -1.276996      0     1    0        1         0  \n",
       "2   -1.043907      0     1    0        1         0  \n",
       "3   -0.915383      0     1    0        1         0  \n",
       "4    0.196447      0     1    1        0         1  \n",
       "..        ...    ...   ...  ...      ...       ...  \n",
       "111 -1.416687      0     1    0        2         0  \n",
       "112 -1.125727      0     1    0        1         1  \n",
       "113 -0.492238      0     1    0        1         1  \n",
       "114 -1.392075      0     1    0        1         0  \n",
       "115 -0.181457      0     1    0        0         1  \n",
       "\n",
       "[116 rows x 11 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predecir VAD usando el modelo entrenado\n",
    "def predict_vad(df1, text_processed1):\n",
    "    model.eval()  # Cambiar el modelo a modo evaluación\n",
    "    input_ids, attention_masks = BERT_tokenization(df1, text_processed1)  # Tokenización de los textos\n",
    "\n",
    "    dataset = TensorDataset(input_ids, attention_masks)\n",
    "    dataloader = DataLoader(dataset, sampler=SequentialSampler(dataset), batch_size=batch_size)\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Predicting\"):\n",
    "        b_input_ids, b_attention_mask = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, attention_mask=b_attention_mask)\n",
    "            logits = outputs.logits\n",
    "            predictions.extend(logits.detach().cpu().numpy())\n",
    "\n",
    "    return np.array(predictions)\n",
    "\n",
    "# Obtener predicciones\n",
    "predictions = predict_vad(df1, \"text\")\n",
    "\n",
    "# Crear un DataFrame con las predicciones de VAD\n",
    "pred_df1 = df1.copy()\n",
    "pred_df1[[\"V_pred\", \"A_pred\", \"D_pred\"]] = predictions\n",
    "\n",
    "# Decodificar las predicciones de VAD a intensidades emocionales\n",
    "emotion_labels = list(vad_values2.keys())\n",
    "vad_matrix = np.array([[vad[\"V\"], vad[\"A\"], vad[\"D\"]] for vad in vad_values2.values()])\n",
    "\n",
    "def decode_vad_to_intensities(v, a, d):\n",
    "    input_vad = np.array([v, a, d])\n",
    "    distances = np.linalg.norm(vad_matrix - input_vad, axis=1)\n",
    "\n",
    "    # Invertir las distancias para calcular intensidades proporcionales\n",
    "    max_distance = np.max(distances)\n",
    "    inverted_distances = max_distance - distances\n",
    "    intensities = (inverted_distances / inverted_distances.sum()) * 3  # Escalar a 0-3\n",
    "    intensities = np.round(intensities).astype(int)  # Redondear a enteros\n",
    "    return pd.Series(intensities, index=emotion_labels)\n",
    "\n",
    "# Aplicar la decodificación a las predicciones\n",
    "decoded_intensities = pred_df1.apply(\n",
    "    lambda row: decode_vad_to_intensities(row[\"V_pred\"], row[\"A_pred\"], row[\"D_pred\"]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Añadir las intensidades decodificadas al DataFrame\n",
    "for emotion in emotion_labels:\n",
    "    pred_df1[emotion] = decoded_intensities[emotion]\n",
    "\n",
    "# Guardar el DataFrame con las predicciones y las intensidades decodificadas\n",
    "pred_df1.to_csv(\"predicciones_decodificadas.csv\", index=False)\n",
    "print(\"Archivo 'predicciones_decodificadas.csv' con intensidades decodificadas generado exitosamente.\")\n",
    "\n",
    "# Visualizar las primeras filas del DataFrame resultante\n",
    "pred_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54cd48f4-3d31-4529-a3fa-dd7db466874c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   id                                    text_processed1  \\\n",
      "0  eng_dev_track2_001  i have a floor shift in the morning hopefully ...   \n",
      "1  eng_dev_track2_002  what is it about this winter that is making me...   \n",
      "2  eng_dev_track2_003      longest most awkward drive i have ever taken.   \n",
      "3  eng_dev_track2_004                     i know not why i wipe my face.   \n",
      "4  eng_dev_track2_005  and i laughed like this garhahagar because my ...   \n",
      "\n",
      "   Anger  Fear  Joy  Sadness  Surprise  \n",
      "0      0     1    0        1         1  \n",
      "1      0     1    0        1         0  \n",
      "2      0     1    0        1         0  \n",
      "3      0     1    0        1         0  \n",
      "4      0     1    1        0         1  \n"
     ]
    }
   ],
   "source": [
    "# Lista de columnas a excluir\n",
    "columns_to_exclude = ['V_pred', 'A_pred', 'D_pred','text']  # Reemplaza con los nombres de las columnas que deseas excluir\n",
    "\n",
    "# Crear un nuevo DataFrame sin las columnas especificadas\n",
    "pred_df_filtered = pred_df1.drop(columns=columns_to_exclude)\n",
    "\n",
    "# Verificar el resultado\n",
    "print(pred_df_filtered.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7876cc57-2c54-4e63-9a56-17a4522a3cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El archivo 'pred_eng.csv' se ha guardado con las columnas reorganizadas.\n"
     ]
    }
   ],
   "source": [
    "# Reorganizar las columnas\n",
    "columns_order = ['id','Anger', 'Fear', 'Joy', 'Sadness', 'Surprise']\n",
    "pred_df_filtered = pred_df_filtered[columns_order]\n",
    "\n",
    "# Guardar el DataFrame reorganizado en un nuevo archivo CSV\n",
    "pred_df_filtered.to_csv(\"pred_eng_b.csv\", index=False)\n",
    "\n",
    "# Confirmar que el archivo ha sido guardado\n",
    "print(\"El archivo 'pred_eng.csv' se ha guardado con las columnas reorganizadas.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6361b89b-5061-440f-8086-30637aa74dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>anger</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eng_test_track_b_00001</td>\n",
       "      <td>/ o \\ So today I went in for a new exam with D...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eng_test_track_b_00002</td>\n",
       "      <td>The image I have in my mind is this: a group o...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eng_test_track_b_00003</td>\n",
       "      <td>I slammed my fist against the door and yelled,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eng_test_track_b_00004</td>\n",
       "      <td>I could not unbend my knees.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eng_test_track_b_00005</td>\n",
       "      <td>I spent the night at the hotel, mostly hanging...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2762</th>\n",
       "      <td>eng_test_track_b_02763</td>\n",
       "      <td>Better late then never!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2763</th>\n",
       "      <td>eng_test_track_b_02764</td>\n",
       "      <td>In the last three weeks, I have started lookin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2764</th>\n",
       "      <td>eng_test_track_b_02765</td>\n",
       "      <td>But I never fell out, so it wasn't a problem.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2765</th>\n",
       "      <td>eng_test_track_b_02766</td>\n",
       "      <td>\" So I will remain positive for as long as I l...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2766</th>\n",
       "      <td>eng_test_track_b_02767</td>\n",
       "      <td>`` Bella my head is fine.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2767 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          id  \\\n",
       "0     eng_test_track_b_00001   \n",
       "1     eng_test_track_b_00002   \n",
       "2     eng_test_track_b_00003   \n",
       "3     eng_test_track_b_00004   \n",
       "4     eng_test_track_b_00005   \n",
       "...                      ...   \n",
       "2762  eng_test_track_b_02763   \n",
       "2763  eng_test_track_b_02764   \n",
       "2764  eng_test_track_b_02765   \n",
       "2765  eng_test_track_b_02766   \n",
       "2766  eng_test_track_b_02767   \n",
       "\n",
       "                                                   text  anger  fear  joy  \\\n",
       "0     / o \\ So today I went in for a new exam with D...    NaN   NaN  NaN   \n",
       "1     The image I have in my mind is this: a group o...    NaN   NaN  NaN   \n",
       "2     I slammed my fist against the door and yelled,...    NaN   NaN  NaN   \n",
       "3                          I could not unbend my knees.    NaN   NaN  NaN   \n",
       "4     I spent the night at the hotel, mostly hanging...    NaN   NaN  NaN   \n",
       "...                                                 ...    ...   ...  ...   \n",
       "2762                            Better late then never!    NaN   NaN  NaN   \n",
       "2763  In the last three weeks, I have started lookin...    NaN   NaN  NaN   \n",
       "2764      But I never fell out, so it wasn't a problem.    NaN   NaN  NaN   \n",
       "2765  \" So I will remain positive for as long as I l...    NaN   NaN  NaN   \n",
       "2766                          `` Bella my head is fine.    NaN   NaN  NaN   \n",
       "\n",
       "      sadness  surprise  \n",
       "0         NaN       NaN  \n",
       "1         NaN       NaN  \n",
       "2         NaN       NaN  \n",
       "3         NaN       NaN  \n",
       "4         NaN       NaN  \n",
       "...       ...       ...  \n",
       "2762      NaN       NaN  \n",
       "2763      NaN       NaN  \n",
       "2764      NaN       NaN  \n",
       "2765      NaN       NaN  \n",
       "2766      NaN       NaN  \n",
       "\n",
       "[2767 rows x 7 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test data\n",
    "import pandas as pd\n",
    "\n",
    "# Cargar el archivo CSV\n",
    "df3 = pd.read_csv(\"eng.csv\")\n",
    "\n",
    "# Verificar la estructura del archivo\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68e469ed-cdf8-4cab-bcee-9cc9588772a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       id                                               text  \\\n",
      "0  eng_test_track_b_00001  / o \\ So today I went in for a new exam with D...   \n",
      "1  eng_test_track_b_00002  The image I have in my mind is this: a group o...   \n",
      "2  eng_test_track_b_00003  I slammed my fist against the door and yelled,...   \n",
      "3  eng_test_track_b_00004                       I could not unbend my knees.   \n",
      "4  eng_test_track_b_00005  I spent the night at the hotel, mostly hanging...   \n",
      "\n",
      "   anger  fear  joy  sadness  surprise  \\\n",
      "0    NaN   NaN  NaN      NaN       NaN   \n",
      "1    NaN   NaN  NaN      NaN       NaN   \n",
      "2    NaN   NaN  NaN      NaN       NaN   \n",
      "3    NaN   NaN  NaN      NaN       NaN   \n",
      "4    NaN   NaN  NaN      NaN       NaN   \n",
      "\n",
      "                                     text_processed3  \n",
      "0  o \\ so today i went in for a new exam with dr....  \n",
      "1  the image i have in my mind is this a group of...  \n",
      "2  i slammed my fist against the door and yelled ...  \n",
      "3                       i could not unbend my knees.  \n",
      "4  i spent the night at the hotel mostly hanging ...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Clase TextProcessing ya definida anteriormente\n",
    "class TextProcessing(object):\n",
    "    name = 'Text Processing'\n",
    "    lang = 'en'\n",
    "\n",
    "    def __init__(self, lang: str = 'en'):\n",
    "        self.lang = lang\n",
    "\n",
    "    @staticmethod\n",
    "    def nlp(text: str) -> list:\n",
    "        try:\n",
    "            list_tagger = []\n",
    "            tp_nlp = TextProcessing.load_spacy(TextProcessing.lang)\n",
    "            doc = tp_nlp(text.lower())\n",
    "            for token in doc:\n",
    "                item = {'text': token.text, 'lemma': token.lemma_, 'pos': token.pos_, 'tag': token.tag_,\n",
    "                        'dep': token.dep_, 'shape': token.shape_, 'is_alpha': token.is_alpha,\n",
    "                        'is_stop': token.is_stop, 'is_digit': token.is_digit, 'is_punct': token.is_punct,\n",
    "                        'syllables': token._.syllables}\n",
    "                list_tagger.append(item)\n",
    "            return list_tagger\n",
    "        except Exception as e:\n",
    "            print('Error nlp: {0}'.format(e))\n",
    "\n",
    "    @staticmethod\n",
    "    def load_spacy(lang: str) -> object:\n",
    "        try:\n",
    "            spacy_model = {'es': 'es_core_news_sm', 'en': 'en_core_web_sm'}\n",
    "            if not spacy.util.is_package(spacy_model[lang]):\n",
    "                spacy.cli.download(spacy_model[lang])\n",
    "\n",
    "            component = spacy.load(spacy_model[lang])\n",
    "            SpacySyllables(component)\n",
    "            component.add_pipe('syllables', last=True)\n",
    "            return component\n",
    "        except Exception as e:\n",
    "            print('Error load spacy: {0}'.format(e))\n",
    "\n",
    "    @staticmethod\n",
    "    def proper_encoding(text: str) -> str:\n",
    "        try:\n",
    "            text = unicodedata.normalize('NFD', text)\n",
    "            text = text.encode('ascii', 'ignore')\n",
    "            return text.decode(\"utf-8\")\n",
    "        except Exception as e:\n",
    "            print('Error proper_encoding: {0}'.format(e))\n",
    "\n",
    "    @staticmethod\n",
    "    def stopwords(text: str) -> str:\n",
    "        try:\n",
    "            nlp = English()\n",
    "            doc = nlp(text)\n",
    "            token_list = [token.text for token in doc]\n",
    "            sentence = []\n",
    "            for word in token_list:\n",
    "                lexeme = nlp.vocab[word]\n",
    "                if not lexeme.is_stop:\n",
    "                    sentence.append(word)\n",
    "            return ' '.join(sentence)\n",
    "        except Exception as e:\n",
    "            print('Error stopwords: {0}'.format(e))\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_patterns(text: str) -> str:\n",
    "        try:\n",
    "            text = re.sub(r'\\©|\\×|\\⇔|\\_|\\»|\\«|\\~|\\#|\\$|\\€|\\Â|\\�|\\¬', '', text)\n",
    "            text = re.sub(r'\\,|\\;|\\:|\\!|\\¡|\\’|\\‘|\\”|\\“|\\\"|\\'|\\`', '', text)\n",
    "            text = re.sub(r'\\}|\\{|\\[|\\]|\\(|\\)|\\<|\\>|\\?|\\¿|\\°|\\|', '', text)\n",
    "            text = re.sub(r'\\/|\\-|\\+|\\*|\\=|\\^|\\%|\\&|\\$', '', text)\n",
    "            text = re.sub(r'\\b\\d+(?:\\.\\d+)?\\s+', '', text)\n",
    "            return text.lower()\n",
    "        except Exception as e:\n",
    "            print('Error remove_patterns: {0}'.format(e))\n",
    "\n",
    "    @staticmethod\n",
    "    def expand_contractions(text: str) -> str:\n",
    "        \"\"\"Expande las contracciones en el texto.\"\"\"\n",
    "        try:\n",
    "            return contractions.fix(text)\n",
    "        except Exception as e:\n",
    "            print('Error expand_contractions: {0}'.format(e))\n",
    "\n",
    "    @staticmethod\n",
    "    def transformer(text: str, stopwords: bool = False) -> str:\n",
    "        try:\n",
    "            text_out = TextProcessing.proper_encoding(text)\n",
    "            text_out = TextProcessing.expand_contractions(text_out)  # Expandimos las contracciones\n",
    "            text_out = text_out.lower()\n",
    "            text_out = re.sub(\"[\\U0001f000-\\U000e007f]\", '[EMOJI]', text_out)\n",
    "            text_out = re.sub(\n",
    "                r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+'\n",
    "                r'|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))',\n",
    "                '[URL]', text_out)\n",
    "            text_out = re.sub(\"@\", '[MENTION]', text_out)\n",
    "            text_out = re.sub(\"#([A-Za-z0-9_]{1,40})\", '[HASTAG]', text_out)\n",
    "            text_out = TextProcessing.remove_patterns(text_out)\n",
    "            text_out = TextProcessing.stopwords(text_out) if stopwords else text_out\n",
    "            text_out = re.sub(r'\\s+', ' ', text_out).strip()\n",
    "            text_out = text_out.rstrip()\n",
    "            return text_out if text_out != ' ' else None\n",
    "        except Exception as e:\n",
    "            print('Error transformer: {0}'.format(e))\n",
    "\n",
    "# Función para aplicar el preprocesamiento al DataFrame\n",
    "def apply_preprocessing_to_df3(df3: pd.DataFrame, column: str) -> pd.DataFrame:\n",
    "    tp = TextProcessing(lang='en')  # Inicializamos el preprocesador en inglés\n",
    "    df3[f'{column}_processed3'] = df3[column].apply(lambda x: tp.transformer(x))\n",
    "    return df3\n",
    "df3 = apply_preprocessing_to_df3(df3, \"text\")\n",
    "print(df3.head()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71d9deca-1a71-4d34-8569-8e6889f6ebb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/jupyterhub/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2690: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Predicting: 100%|██████████| 346/346 [05:23<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo 'predicciones_decodificadas.csv' con intensidades decodificadas generado exitosamente.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>anger</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>text_processed3</th>\n",
       "      <th>V_pred</th>\n",
       "      <th>A_pred</th>\n",
       "      <th>D_pred</th>\n",
       "      <th>Anger</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Surprise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eng_test_track_b_00001</td>\n",
       "      <td>/ o \\ So today I went in for a new exam with D...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>o \\ so today i went in for a new exam with dr....</td>\n",
       "      <td>0.311887</td>\n",
       "      <td>0.511399</td>\n",
       "      <td>-0.875982</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eng_test_track_b_00002</td>\n",
       "      <td>The image I have in my mind is this: a group o...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the image i have in my mind is this a group of...</td>\n",
       "      <td>0.611419</td>\n",
       "      <td>0.864274</td>\n",
       "      <td>-1.139573</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eng_test_track_b_00003</td>\n",
       "      <td>I slammed my fist against the door and yelled,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i slammed my fist against the door and yelled ...</td>\n",
       "      <td>0.035407</td>\n",
       "      <td>1.324540</td>\n",
       "      <td>0.075120</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eng_test_track_b_00004</td>\n",
       "      <td>I could not unbend my knees.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i could not unbend my knees.</td>\n",
       "      <td>0.639134</td>\n",
       "      <td>0.798000</td>\n",
       "      <td>-0.949776</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eng_test_track_b_00005</td>\n",
       "      <td>I spent the night at the hotel, mostly hanging...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i spent the night at the hotel mostly hanging ...</td>\n",
       "      <td>0.720385</td>\n",
       "      <td>0.968216</td>\n",
       "      <td>-0.572761</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2762</th>\n",
       "      <td>eng_test_track_b_02763</td>\n",
       "      <td>Better late then never!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>better late then never</td>\n",
       "      <td>0.384021</td>\n",
       "      <td>0.913803</td>\n",
       "      <td>-0.249709</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2763</th>\n",
       "      <td>eng_test_track_b_02764</td>\n",
       "      <td>In the last three weeks, I have started lookin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>in the last three weeks i have started looking...</td>\n",
       "      <td>2.721167</td>\n",
       "      <td>1.749632</td>\n",
       "      <td>0.911263</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2764</th>\n",
       "      <td>eng_test_track_b_02765</td>\n",
       "      <td>But I never fell out, so it wasn't a problem.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>but i never fell out so it was not a problem.</td>\n",
       "      <td>1.394246</td>\n",
       "      <td>1.041869</td>\n",
       "      <td>0.670498</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2765</th>\n",
       "      <td>eng_test_track_b_02766</td>\n",
       "      <td>\" So I will remain positive for as long as I l...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>so i will remain positive for as long as i live.</td>\n",
       "      <td>1.366620</td>\n",
       "      <td>0.746311</td>\n",
       "      <td>-0.124205</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2766</th>\n",
       "      <td>eng_test_track_b_02767</td>\n",
       "      <td>`` Bella my head is fine.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bella my head is fine.</td>\n",
       "      <td>0.831783</td>\n",
       "      <td>0.421839</td>\n",
       "      <td>-0.738118</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2767 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          id  \\\n",
       "0     eng_test_track_b_00001   \n",
       "1     eng_test_track_b_00002   \n",
       "2     eng_test_track_b_00003   \n",
       "3     eng_test_track_b_00004   \n",
       "4     eng_test_track_b_00005   \n",
       "...                      ...   \n",
       "2762  eng_test_track_b_02763   \n",
       "2763  eng_test_track_b_02764   \n",
       "2764  eng_test_track_b_02765   \n",
       "2765  eng_test_track_b_02766   \n",
       "2766  eng_test_track_b_02767   \n",
       "\n",
       "                                                   text  anger  fear  joy  \\\n",
       "0     / o \\ So today I went in for a new exam with D...    NaN   NaN  NaN   \n",
       "1     The image I have in my mind is this: a group o...    NaN   NaN  NaN   \n",
       "2     I slammed my fist against the door and yelled,...    NaN   NaN  NaN   \n",
       "3                          I could not unbend my knees.    NaN   NaN  NaN   \n",
       "4     I spent the night at the hotel, mostly hanging...    NaN   NaN  NaN   \n",
       "...                                                 ...    ...   ...  ...   \n",
       "2762                            Better late then never!    NaN   NaN  NaN   \n",
       "2763  In the last three weeks, I have started lookin...    NaN   NaN  NaN   \n",
       "2764      But I never fell out, so it wasn't a problem.    NaN   NaN  NaN   \n",
       "2765  \" So I will remain positive for as long as I l...    NaN   NaN  NaN   \n",
       "2766                          `` Bella my head is fine.    NaN   NaN  NaN   \n",
       "\n",
       "      sadness  surprise                                    text_processed3  \\\n",
       "0         NaN       NaN  o \\ so today i went in for a new exam with dr....   \n",
       "1         NaN       NaN  the image i have in my mind is this a group of...   \n",
       "2         NaN       NaN  i slammed my fist against the door and yelled ...   \n",
       "3         NaN       NaN                       i could not unbend my knees.   \n",
       "4         NaN       NaN  i spent the night at the hotel mostly hanging ...   \n",
       "...       ...       ...                                                ...   \n",
       "2762      NaN       NaN                             better late then never   \n",
       "2763      NaN       NaN  in the last three weeks i have started looking...   \n",
       "2764      NaN       NaN      but i never fell out so it was not a problem.   \n",
       "2765      NaN       NaN   so i will remain positive for as long as i live.   \n",
       "2766      NaN       NaN                             bella my head is fine.   \n",
       "\n",
       "        V_pred    A_pred    D_pred  Anger  Fear  Joy  Sadness  Surprise  \n",
       "0     0.311887  0.511399 -0.875982      0     1    0        1         0  \n",
       "1     0.611419  0.864274 -1.139573      0     1    0        1         1  \n",
       "2     0.035407  1.324540  0.075120      1     1    0        0         1  \n",
       "3     0.639134  0.798000 -0.949776      0     1    0        1         1  \n",
       "4     0.720385  0.968216 -0.572761      0     1    0        1         1  \n",
       "...        ...       ...       ...    ...   ...  ...      ...       ...  \n",
       "2762  0.384021  0.913803 -0.249709      1     1    0        1         1  \n",
       "2763  2.721167  1.749632  0.911263      0     1    1        0         1  \n",
       "2764  1.394246  1.041869  0.670498      0     1    1        0         1  \n",
       "2765  1.366620  0.746311 -0.124205      0     1    1        1         1  \n",
       "2766  0.831783  0.421839 -0.738118      0     1    0        1         0  \n",
       "\n",
       "[2767 rows x 16 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Predecir VAD usando el modelo entrenado\n",
    "def predict_vad(df3,text_processed3):\n",
    "    model.eval()  # Cambiar el modelo a modo evaluación\n",
    "    input_ids, attention_masks = BERT_tokenization(df3, text_processed3)  # Tokenización de los textos\n",
    "\n",
    "    dataset = TensorDataset(input_ids, attention_masks)\n",
    "    dataloader = DataLoader(dataset, sampler=SequentialSampler(dataset), batch_size=batch_size)\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Predicting\"):\n",
    "        b_input_ids, b_attention_mask = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, attention_mask=b_attention_mask)\n",
    "            logits = outputs.logits\n",
    "            predictions.extend(logits.detach().cpu().numpy())\n",
    "\n",
    "    return np.array(predictions)\n",
    "\n",
    "# Obtener predicciones\n",
    "predictions = predict_vad(df3, \"text_processed3\")\n",
    "\n",
    "# Crear un DataFrame con las predicciones de VAD\n",
    "pred_df3 = df3.copy()\n",
    "pred_df3[[\"V_pred\", \"A_pred\", \"D_pred\"]] = predictions\n",
    "\n",
    "# Decodificar las predicciones de VAD a intensidades emocionales\n",
    "emotion_labels = list(vad_values2.keys())\n",
    "vad_matrix = np.array([[vad[\"V\"], vad[\"A\"], vad[\"D\"]] for vad in vad_values2.values()])\n",
    "\n",
    "def decode_vad_to_intensities(v, a, d):\n",
    "    input_vad = np.array([v, a, d])\n",
    "    distances = np.linalg.norm(vad_matrix - input_vad, axis=1)\n",
    "\n",
    "    # Invertir las distancias para calcular intensidades proporcionales\n",
    "    max_distance = np.max(distances)\n",
    "    inverted_distances = max_distance - distances\n",
    "    intensities = (inverted_distances / inverted_distances.sum()) * 3  # Escalar a 0-3\n",
    "    intensities = np.round(intensities).astype(int)  # Redondear a enteros\n",
    "    return pd.Series(intensities, index=emotion_labels)\n",
    "\n",
    "# Aplicar la decodificación a las predicciones\n",
    "decoded_intensities = pred_df3.apply(\n",
    "    lambda row: decode_vad_to_intensities(row[\"V_pred\"], row[\"A_pred\"], row[\"D_pred\"]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Añadir las intensidades decodificadas al DataFrame\n",
    "for emotion in emotion_labels:\n",
    "    pred_df3[emotion] = decoded_intensities[emotion]\n",
    "\n",
    "# Guardar el DataFrame con las predicciones y las intensidades decodificadas\n",
    "pred_df3.to_csv(\"predicciones_decodificadas.csv\", index=False)\n",
    "print(\"Archivo 'predicciones_decodificadas.csv' con intensidades decodificadas generado exitosamente.\")\n",
    "\n",
    "# Visualizar las primeras filas del DataFrame resultante\n",
    "pred_df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c627898e-a3e3-4ac8-a349-94359f7a2e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      id                                               text  \\\n",
      "0   eng_train_track2_001       None of us has mentioned the incident since.   \n",
      "1   eng_train_track2_015                       So... for reasons unknown...   \n",
      "8   eng_train_track2_022  Later when we got home she saw all her present...   \n",
      "9   eng_train_track2_023                              Was I drunk or a kid?   \n",
      "10  eng_train_track2_024  I farted and a little nugget fell out onto the...   \n",
      "\n",
      "    Joy  Fear  Anger  Sadness  Surprise  \n",
      "0   0.0   1.0    0.0      2.0       1.0  \n",
      "1   0.0   1.0    0.0      0.0       2.0  \n",
      "8   2.0   0.0    0.0      0.0       1.0  \n",
      "9   0.0   2.0    0.0      0.0       2.0  \n",
      "10  1.0   0.0    0.0      0.0       2.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar el archivo CSV con opciones para manejar comillas y delimitadores\n",
    "df4 = pd.read_csv(\n",
    "    \"eng2_train.csv\",\n",
    "    delimiter=\",\",  # Asegura que el delimitador sea la coma\n",
    "    quotechar='\"',  # Manejo adecuado de comillas dobles\n",
    "    encoding=\"utf-8\",  # Usa UTF-8 para evitar problemas de caracteres especiales\n",
    "    skip_blank_lines=True,  # Ignora líneas en blanco\n",
    "    on_bad_lines=\"skip\"  # Evita errores por líneas mal formateadas\n",
    ")\n",
    "\n",
    "# Verificar si hay valores NaN\n",
    "df4.dropna(inplace=True)  # Elimina filas con valores NaN generados por errores de lectura\n",
    "\n",
    "# Mostrar las primeras filas para verificar la estructura\n",
    "print(df4.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "841e2333-8cc4-4fa8-9c61-fbb369dd6805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      id                                               text  \\\n",
      "0   eng_train_track2_001       None of us has mentioned the incident since.   \n",
      "1   eng_train_track2_015                       So... for reasons unknown...   \n",
      "8   eng_train_track2_022  Later when we got home she saw all her present...   \n",
      "9   eng_train_track2_023                              Was I drunk or a kid?   \n",
      "10  eng_train_track2_024  I farted and a little nugget fell out onto the...   \n",
      "\n",
      "    Joy  Fear  Anger  Sadness  Surprise  \\\n",
      "0   0.0   1.0    0.0      2.0       1.0   \n",
      "1   0.0   1.0    0.0      0.0       2.0   \n",
      "8   2.0   0.0    0.0      0.0       1.0   \n",
      "9   0.0   2.0    0.0      0.0       2.0   \n",
      "10  1.0   0.0    0.0      0.0       2.0   \n",
      "\n",
      "                                       text_processed  \n",
      "0        none of us has mentioned the incident since.  \n",
      "1                        so... for reasons unknown...  \n",
      "8   later when we got home she saw all her present...  \n",
      "9                                was i drunk or a kid  \n",
      "10  i farted and a little nugget fell out onto the...  \n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "import nltk\n",
    "import unicodedata\n",
    "import requests\n",
    "from spacy_syllables import SpacySyllables\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import TweetTokenizer\n",
    "from spacy.lang.es import Spanish\n",
    "from spacy.lang.en import English\n",
    "from nltk.util import ngrams\n",
    "import pandas as pd\n",
    "import contractions  # Importamos la librería para expandir contracciones\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "# Clase TextProcessing ya definida anteriormente\n",
    "class TextProcessing(object):\n",
    "    name = 'Text Processing'\n",
    "    lang = 'en'\n",
    "\n",
    "    def __init__(self, lang: str = 'en'):\n",
    "        self.lang = lang\n",
    "\n",
    "    @staticmethod\n",
    "    def nlp(text: str) -> list:\n",
    "        try:\n",
    "            list_tagger = []\n",
    "            tp_nlp = TextProcessing.load_spacy(TextProcessing.lang)\n",
    "            doc = tp_nlp(text.lower())\n",
    "            for token in doc:\n",
    "                item = {'text': token.text, 'lemma': token.lemma_, 'pos': token.pos_, 'tag': token.tag_,\n",
    "                        'dep': token.dep_, 'shape': token.shape_, 'is_alpha': token.is_alpha,\n",
    "                        'is_stop': token.is_stop, 'is_digit': token.is_digit, 'is_punct': token.is_punct,\n",
    "                        'syllables': token._.syllables}\n",
    "                list_tagger.append(item)\n",
    "            return list_tagger\n",
    "        except Exception as e:\n",
    "            print('Error nlp: {0}'.format(e))\n",
    "\n",
    "    @staticmethod\n",
    "    def load_spacy(lang: str) -> object:\n",
    "        try:\n",
    "            spacy_model = {'es': 'es_core_news_sm', 'en': 'en_core_web_sm'}\n",
    "            if not spacy.util.is_package(spacy_model[lang]):\n",
    "                spacy.cli.download(spacy_model[lang])\n",
    "\n",
    "            component = spacy.load(spacy_model[lang])\n",
    "            SpacySyllables(component)\n",
    "            component.add_pipe('syllables', last=True)\n",
    "            return component\n",
    "        except Exception as e:\n",
    "            print('Error load spacy: {0}'.format(e))\n",
    "\n",
    "    @staticmethod\n",
    "    def proper_encoding(text: str) -> str:\n",
    "        try:\n",
    "            text = unicodedata.normalize('NFD', text)\n",
    "            text = text.encode('ascii', 'ignore')\n",
    "            return text.decode(\"utf-8\")\n",
    "        except Exception as e:\n",
    "            print('Error proper_encoding: {0}'.format(e))\n",
    "\n",
    "    @staticmethod\n",
    "    def stopwords(text: str) -> str:\n",
    "        try:\n",
    "            nlp = English()\n",
    "            doc = nlp(text)\n",
    "            token_list = [token.text for token in doc]\n",
    "            sentence = []\n",
    "            for word in token_list:\n",
    "                lexeme = nlp.vocab[word]\n",
    "                if not lexeme.is_stop:\n",
    "                    sentence.append(word)\n",
    "            return ' '.join(sentence)\n",
    "        except Exception as e:\n",
    "            print('Error stopwords: {0}'.format(e))\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_patterns(text: str) -> str:\n",
    "        try:\n",
    "            text = re.sub(r'\\©|\\×|\\⇔|\\_|\\»|\\«|\\~|\\#|\\$|\\€|\\Â|\\�|\\¬', '', text)\n",
    "            text = re.sub(r'\\,|\\;|\\:|\\!|\\¡|\\’|\\‘|\\”|\\“|\\\"|\\'|\\`', '', text)\n",
    "            text = re.sub(r'\\}|\\{|\\[|\\]|\\(|\\)|\\<|\\>|\\?|\\¿|\\°|\\|', '', text)\n",
    "            text = re.sub(r'\\/|\\-|\\+|\\*|\\=|\\^|\\%|\\&|\\$', '', text)\n",
    "            text = re.sub(r'\\b\\d+(?:\\.\\d+)?\\s+', '', text)\n",
    "            return text.lower()\n",
    "        except Exception as e:\n",
    "            print('Error remove_patterns: {0}'.format(e))\n",
    "\n",
    "    @staticmethod\n",
    "    def expand_contractions(text: str) -> str:\n",
    "        \"\"\"Expande las contracciones en el texto.\"\"\"\n",
    "        try:\n",
    "            return contractions.fix(text)\n",
    "        except Exception as e:\n",
    "            print('Error expand_contractions: {0}'.format(e))\n",
    "\n",
    "    @staticmethod\n",
    "    def transformer(text: str, stopwords: bool = False) -> str:\n",
    "        try:\n",
    "            text_out = TextProcessing.proper_encoding(text)\n",
    "            text_out = TextProcessing.expand_contractions(text_out)  # Expandimos las contracciones\n",
    "            text_out = text_out.lower()\n",
    "            text_out = re.sub(\"[\\U0001f000-\\U000e007f]\", '[EMOJI]', text_out)\n",
    "            text_out = re.sub(\n",
    "                r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+'\n",
    "                r'|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))',\n",
    "                '[URL]', text_out)\n",
    "            text_out = re.sub(\"@\", '[MENTION]', text_out)\n",
    "            text_out = re.sub(\"#([A-Za-z0-9_]{1,40})\", '[HASTAG]', text_out)\n",
    "            text_out = TextProcessing.remove_patterns(text_out)\n",
    "            text_out = TextProcessing.stopwords(text_out) if stopwords else text_out\n",
    "            text_out = re.sub(r'\\s+', ' ', text_out).strip()\n",
    "            text_out = text_out.rstrip()\n",
    "            return text_out if text_out != ' ' else None\n",
    "        except Exception as e:\n",
    "            print('Error transformer: {0}'.format(e))\n",
    "\n",
    "\n",
    "# Función para aplicar el preprocesamiento al DataFrame\n",
    "def apply_preprocessing_to_df(df4: pd.DataFrame, column: str) -> pd.DataFrame:\n",
    "    tp = TextProcessing(lang='en')  # Inicializamos el preprocesador en inglés\n",
    "    df4[f'{column}_processed'] = df4[column].apply(lambda x: tp.transformer(x))\n",
    "    return df4\n",
    "df4 = apply_preprocessing_to_df(df4, \"text\")\n",
    "print(df4.head()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cbe0951a-3a51-429d-901f-a09f496fd81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/jupyterhub/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2690: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Predicting: 100%|██████████| 1/1 [00:00<00:00,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo 'predicciones_decodificadas.csv' con intensidades decodificadas generado exitosamente.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Anger</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Surprise</th>\n",
       "      <th>text_processed</th>\n",
       "      <th>V_pred</th>\n",
       "      <th>A_pred</th>\n",
       "      <th>D_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eng_train_track2_001</td>\n",
       "      <td>None of us has mentioned the incident since.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>none of us has mentioned the incident since.</td>\n",
       "      <td>0.399733</td>\n",
       "      <td>-0.068715</td>\n",
       "      <td>-0.896017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eng_train_track2_015</td>\n",
       "      <td>So... for reasons unknown...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>so... for reasons unknown...</td>\n",
       "      <td>0.876226</td>\n",
       "      <td>1.368630</td>\n",
       "      <td>-0.435247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>eng_train_track2_022</td>\n",
       "      <td>Later when we got home she saw all her present...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>later when we got home she saw all her present...</td>\n",
       "      <td>2.949194</td>\n",
       "      <td>1.530099</td>\n",
       "      <td>1.310039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>eng_train_track2_023</td>\n",
       "      <td>Was I drunk or a kid?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>was i drunk or a kid</td>\n",
       "      <td>1.122741</td>\n",
       "      <td>1.695458</td>\n",
       "      <td>-0.012874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>eng_train_track2_024</td>\n",
       "      <td>I farted and a little nugget fell out onto the...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>i farted and a little nugget fell out onto the...</td>\n",
       "      <td>2.812530</td>\n",
       "      <td>1.659924</td>\n",
       "      <td>1.206395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>eng_train_track2_026</td>\n",
       "      <td>Never found anything... kind of spooky.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>never found anything... kind of spooky.</td>\n",
       "      <td>1.487133</td>\n",
       "      <td>1.454574</td>\n",
       "      <td>-0.584707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>eng_train_track2_028</td>\n",
       "      <td>We cuddled on the couch whilst watching TV.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>we cuddled on the couch whilst watching tv.</td>\n",
       "      <td>2.639673</td>\n",
       "      <td>1.159558</td>\n",
       "      <td>1.436588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>eng_train_track2_2768</td>\n",
       "      <td>I stopped a couple times to stretch out my cal...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>i stopped a couple times to stretch out my cal...</td>\n",
       "      <td>1.895036</td>\n",
       "      <td>1.227654</td>\n",
       "      <td>0.126750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id                                               text  \\\n",
       "0    eng_train_track2_001       None of us has mentioned the incident since.   \n",
       "1    eng_train_track2_015                       So... for reasons unknown...   \n",
       "8    eng_train_track2_022  Later when we got home she saw all her present...   \n",
       "9    eng_train_track2_023                              Was I drunk or a kid?   \n",
       "10   eng_train_track2_024  I farted and a little nugget fell out onto the...   \n",
       "12   eng_train_track2_026            Never found anything... kind of spooky.   \n",
       "14   eng_train_track2_028        We cuddled on the couch whilst watching TV.   \n",
       "16  eng_train_track2_2768  I stopped a couple times to stretch out my cal...   \n",
       "\n",
       "    Joy  Fear  Anger  Sadness  Surprise  \\\n",
       "0     0     1      0        2         0   \n",
       "1     0     2      0        0         1   \n",
       "8     1     1      0        0         1   \n",
       "9     1     1      0        0         1   \n",
       "10    1     1      0        0         1   \n",
       "12    1     1      0        0         1   \n",
       "14    1     1      0        0         1   \n",
       "16    1     1      0        0         1   \n",
       "\n",
       "                                       text_processed    V_pred    A_pred  \\\n",
       "0        none of us has mentioned the incident since.  0.399733 -0.068715   \n",
       "1                        so... for reasons unknown...  0.876226  1.368630   \n",
       "8   later when we got home she saw all her present...  2.949194  1.530099   \n",
       "9                                was i drunk or a kid  1.122741  1.695458   \n",
       "10  i farted and a little nugget fell out onto the...  2.812530  1.659924   \n",
       "12            never found anything... kind of spooky.  1.487133  1.454574   \n",
       "14        we cuddled on the couch whilst watching tv.  2.639673  1.159558   \n",
       "16  i stopped a couple times to stretch out my cal...  1.895036  1.227654   \n",
       "\n",
       "      D_pred  \n",
       "0  -0.896017  \n",
       "1  -0.435247  \n",
       "8   1.310039  \n",
       "9  -0.012874  \n",
       "10  1.206395  \n",
       "12 -0.584707  \n",
       "14  1.436588  \n",
       "16  0.126750  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Predecir VAD usando el modelo entrenado\n",
    "def predict_vad(df4,text_processed):\n",
    "    model.eval()  # Cambiar el modelo a modo evaluación\n",
    "    input_ids, attention_masks = BERT_tokenization(df4, text_processed)  # Tokenización de los textos\n",
    "\n",
    "    dataset = TensorDataset(input_ids, attention_masks)\n",
    "    dataloader = DataLoader(dataset, sampler=SequentialSampler(dataset), batch_size=batch_size)\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Predicting\"):\n",
    "        b_input_ids, b_attention_mask = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, attention_mask=b_attention_mask)\n",
    "            logits = outputs.logits\n",
    "            predictions.extend(logits.detach().cpu().numpy())\n",
    "\n",
    "    return np.array(predictions)\n",
    "# Obtener predicciones\n",
    "predictions = predict_vad(df4, \"text_processed\")\n",
    "\n",
    "# Crear un DataFrame con las predicciones de VAD\n",
    "pred_df4 = df4.copy()\n",
    "pred_df4[[\"V_pred\", \"A_pred\", \"D_pred\"]] = predictions\n",
    "\n",
    "# Decodificar las predicciones de VAD a intensidades emocionales\n",
    "emotion_labels = list(vad_values2.keys())\n",
    "vad_matrix = np.array([[vad[\"V\"], vad[\"A\"], vad[\"D\"]] for vad in vad_values2.values()])\n",
    "\n",
    "def decode_vad_to_intensities(v, a, d):\n",
    "    input_vad = np.array([v, a, d])\n",
    "    distances = np.linalg.norm(vad_matrix - input_vad, axis=1)\n",
    "\n",
    "    # Invertir las distancias para calcular intensidades proporcionales\n",
    "    max_distance = np.max(distances)\n",
    "    inverted_distances = max_distance - distances\n",
    "    intensities = (inverted_distances / inverted_distances.sum()) * 3  # Escalar a 0-3\n",
    "    intensities = np.round(intensities).astype(int)  # Redondear a enteros\n",
    "    return pd.Series(intensities, index=emotion_labels)\n",
    "\n",
    "# Aplicar la decodificación a las predicciones\n",
    "decoded_intensities = pred_df4.apply(\n",
    "    lambda row: decode_vad_to_intensities(row[\"V_pred\"], row[\"A_pred\"], row[\"D_pred\"]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Añadir las intensidades decodificadas al DataFrame\n",
    "for emotion in emotion_labels:\n",
    "    pred_df4[emotion] = decoded_intensities[emotion]\n",
    "\n",
    "# Guardar el DataFrame con las predicciones y las intensidades decodificadas\n",
    "pred_df4.to_csv(\"predicciones_decodificadas.csv\", index=False)\n",
    "print(\"Archivo 'predicciones_decodificadas.csv' con intensidades decodificadas generado exitosamente.\")\n",
    "\n",
    "# Visualizar las primeras filas del DataFrame resultante\n",
    "pred_df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e576c1dc-4ce9-473f-8de5-1b7f7aef0828",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d86b7f-5256-430b-8736-7f6d0152f37b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
