{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "orda9lnlBAjt",
    "outputId": "5d8e41c6-1d4a-49bf-d100-850dbc0418e8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Anger</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Surprise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eng_train_track2_001</td>\n",
       "      <td>None of us has mentioned the incident since.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eng_train_track2_002</td>\n",
       "      <td>I was 7 and woke up early, so I went to the ba...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eng_train_track2_003</td>\n",
       "      <td>By that point I felt like someone was stabbing...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eng_train_track2_004</td>\n",
       "      <td>watching her leave with dudes drove me crazy.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eng_train_track2_005</td>\n",
       "      <td>`` My eyes widened.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2763</th>\n",
       "      <td>eng_train_track2_2764</td>\n",
       "      <td>My face is cold, and my hands are guilty.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2764</th>\n",
       "      <td>eng_train_track2_2765</td>\n",
       "      <td>I remembered how I dragged his box into the be...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2765</th>\n",
       "      <td>eng_train_track2_2766</td>\n",
       "      <td>As I walked in the door she came around the co...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2766</th>\n",
       "      <td>eng_train_track2_2767</td>\n",
       "      <td>They kept me at the hospital for 24 hours-and ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2767</th>\n",
       "      <td>eng_train_track2_2768</td>\n",
       "      <td>I stopped a couple times to stretch out my cal...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2768 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id  \\\n",
       "0      eng_train_track2_001   \n",
       "1      eng_train_track2_002   \n",
       "2      eng_train_track2_003   \n",
       "3      eng_train_track2_004   \n",
       "4      eng_train_track2_005   \n",
       "...                     ...   \n",
       "2763  eng_train_track2_2764   \n",
       "2764  eng_train_track2_2765   \n",
       "2765  eng_train_track2_2766   \n",
       "2766  eng_train_track2_2767   \n",
       "2767  eng_train_track2_2768   \n",
       "\n",
       "                                                   text  Joy  Fear  Anger  \\\n",
       "0          None of us has mentioned the incident since.    0     1      0   \n",
       "1     I was 7 and woke up early, so I went to the ba...    1     0      0   \n",
       "2     By that point I felt like someone was stabbing...    0     3      0   \n",
       "3         watching her leave with dudes drove me crazy.    0     1      3   \n",
       "4                                   `` My eyes widened.    0     1      0   \n",
       "...                                                 ...  ...   ...    ...   \n",
       "2763          My face is cold, and my hands are guilty.    0     1      0   \n",
       "2764  I remembered how I dragged his box into the be...    1     0      0   \n",
       "2765  As I walked in the door she came around the co...    3     0      0   \n",
       "2766  They kept me at the hospital for 24 hours-and ...    0     1      0   \n",
       "2767  I stopped a couple times to stretch out my cal...    0     0      0   \n",
       "\n",
       "      Sadness  Surprise  \n",
       "0           2         1  \n",
       "1           0         0  \n",
       "2           0         0  \n",
       "3           1         0  \n",
       "4           0         2  \n",
       "...       ...       ...  \n",
       "2763        1         0  \n",
       "2764        0         0  \n",
       "2765        0         1  \n",
       "2766        1         0  \n",
       "2767        0         0  \n",
       "\n",
       "[2768 rows x 7 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar el archivo CSV\n",
    "df = pd.read_csv(\"eng_train.csv\")\n",
    "\n",
    "# Verificar la estructura del archivo\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JgQB31qVWACv"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "import nltk\n",
    "import unicodedata\n",
    "import requests\n",
    "from spacy_syllables import SpacySyllables\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import TweetTokenizer\n",
    "from spacy.lang.es import Spanish\n",
    "from spacy.lang.en import English\n",
    "from nltk.util import ngrams\n",
    "import pandas as pd\n",
    "import contractions  # Importamos la librería para expandir contracciones\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "\n",
    "#df = pd.DataFrame(df_final)\n",
    "\n",
    "# Clase TextProcessing ya definida anteriormente\n",
    "class TextProcessing(object):\n",
    "    name = 'Text Processing'\n",
    "    lang = 'en'\n",
    "\n",
    "    def __init__(self, lang: str = 'en'):\n",
    "        self.lang = lang\n",
    "\n",
    "    @staticmethod\n",
    "    def nlp(text: str) -> list:\n",
    "        try:\n",
    "            list_tagger = []\n",
    "            tp_nlp = TextProcessing.load_spacy(TextProcessing.lang)\n",
    "            doc = tp_nlp(text.lower())\n",
    "            for token in doc:\n",
    "                item = {'text': token.text, 'lemma': token.lemma_, 'pos': token.pos_, 'tag': token.tag_,\n",
    "                        'dep': token.dep_, 'shape': token.shape_, 'is_alpha': token.is_alpha,\n",
    "                        'is_stop': token.is_stop, 'is_digit': token.is_digit, 'is_punct': token.is_punct,\n",
    "                        'syllables': token._.syllables}\n",
    "                list_tagger.append(item)\n",
    "            return list_tagger\n",
    "        except Exception as e:\n",
    "            print('Error nlp: {0}'.format(e))\n",
    "\n",
    "    @staticmethod\n",
    "    def load_spacy(lang: str) -> object:\n",
    "        try:\n",
    "            spacy_model = {'es': 'es_core_news_sm', 'en': 'en_core_web_sm'}\n",
    "            if not spacy.util.is_package(spacy_model[lang]):\n",
    "                spacy.cli.download(spacy_model[lang])\n",
    "\n",
    "            component = spacy.load(spacy_model[lang])\n",
    "            SpacySyllables(component)\n",
    "            component.add_pipe('syllables', last=True)\n",
    "            return component\n",
    "        except Exception as e:\n",
    "            print('Error load spacy: {0}'.format(e))\n",
    "\n",
    "    @staticmethod\n",
    "    def proper_encoding(text: str) -> str:\n",
    "        try:\n",
    "            text = unicodedata.normalize('NFD', text)\n",
    "            text = text.encode('ascii', 'ignore')\n",
    "            return text.decode(\"utf-8\")\n",
    "        except Exception as e:\n",
    "            print('Error proper_encoding: {0}'.format(e))\n",
    "\n",
    "    @staticmethod\n",
    "    def stopwords(text: str) -> str:\n",
    "        try:\n",
    "            nlp = English()\n",
    "            doc = nlp(text)\n",
    "            token_list = [token.text for token in doc]\n",
    "            sentence = []\n",
    "            for word in token_list:\n",
    "                lexeme = nlp.vocab[word]\n",
    "                if not lexeme.is_stop:\n",
    "                    sentence.append(word)\n",
    "            return ' '.join(sentence)\n",
    "        except Exception as e:\n",
    "            print('Error stopwords: {0}'.format(e))\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_patterns(text: str) -> str:\n",
    "        try:\n",
    "            text = re.sub(r'\\©|\\×|\\⇔|\\_|\\»|\\«|\\~|\\#|\\$|\\€|\\Â|\\�|\\¬', '', text)\n",
    "            text = re.sub(r'\\,|\\;|\\:|\\!|\\¡|\\’|\\‘|\\”|\\“|\\\"|\\'|\\`', '', text)\n",
    "            text = re.sub(r'\\}|\\{|\\[|\\]|\\(|\\)|\\<|\\>|\\?|\\¿|\\°|\\|', '', text)\n",
    "            text = re.sub(r'\\/|\\-|\\+|\\*|\\=|\\^|\\%|\\&|\\$', '', text)\n",
    "            text = re.sub(r'\\b\\d+(?:\\.\\d+)?\\s+', '', text)\n",
    "            return text.lower()\n",
    "        except Exception as e:\n",
    "            print('Error remove_patterns: {0}'.format(e))\n",
    "\n",
    "    @staticmethod\n",
    "    def expand_contractions(text: str) -> str:\n",
    "        \"\"\"Expande las contracciones en el texto.\"\"\"\n",
    "        try:\n",
    "            return contractions.fix(text)\n",
    "        except Exception as e:\n",
    "            print('Error expand_contractions: {0}'.format(e))\n",
    "\n",
    "    @staticmethod\n",
    "    def transformer(text: str, stopwords: bool = False) -> str:\n",
    "        try:\n",
    "            text_out = TextProcessing.proper_encoding(text)\n",
    "            text_out = TextProcessing.expand_contractions(text_out)  # Expandimos las contracciones\n",
    "            text_out = text_out.lower()\n",
    "            text_out = re.sub(\"[\\U0001f000-\\U000e007f]\", '[EMOJI]', text_out)\n",
    "            text_out = re.sub(\n",
    "                r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+'\n",
    "                r'|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))',\n",
    "                '[URL]', text_out)\n",
    "            text_out = re.sub(\"@\", '[MENTION]', text_out)\n",
    "            text_out = re.sub(\"#([A-Za-z0-9_]{1,40})\", '[HASTAG]', text_out)\n",
    "            text_out = TextProcessing.remove_patterns(text_out)\n",
    "            text_out = TextProcessing.stopwords(text_out) if stopwords else text_out\n",
    "            text_out = re.sub(r'\\s+', ' ', text_out).strip()\n",
    "            text_out = text_out.rstrip()\n",
    "            return text_out if text_out != ' ' else None\n",
    "        except Exception as e:\n",
    "            print('Error transformer: {0}'.format(e))\n",
    "\n",
    "\n",
    "# Función para aplicar el preprocesamiento al DataFrame\n",
    "def apply_preprocessing_to_df(df: pd.DataFrame, column: str) -> pd.DataFrame:\n",
    "    tp = TextProcessing(lang='en')  # Inicializamos el preprocesador en inglés\n",
    "    df[f'{column}_processed'] = df[column].apply(lambda x: tp.transformer(x))\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vad_values2 = {\n",
    "    \"Anger\": {\"V\": 0.167, \"A\": 0.865, \"D\": 0.657},\n",
    "    \"Fear\": {\"V\": 0.73, \"A\": 0.840, \"D\": 0.293},\n",
    "    \"Joy\": {\"V\": 0.980, \"A\": 0.824, \"D\": 0.794},\n",
    "    \"Sadness\": {\"V\": 0.52, \"A\": 0.288, \"D\": 0.164},\n",
    "    \"Surprise\": {\"V\": 0.875, \"A\": 0.875, \"D\": 0.562},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         id  \\\n",
      "2124  eng_train_track2_2125   \n",
      "2716  eng_train_track2_2717   \n",
      "2232  eng_train_track2_2233   \n",
      "261    eng_train_track2_262   \n",
      "2059  eng_train_track2_2060   \n",
      "\n",
      "                                                   text  Joy  Fear  Anger  \\\n",
      "2124  26 January 2011 @ 04:45 pm Boys & Girls 718 Cr...    0     2      0   \n",
      "2716  We headed north on a sunny Saturday morning in...    1     0      0   \n",
      "2232  I looked down to find five small white arrows ...    0     3      0   \n",
      "261                         I've never gone back there.    0     2      0   \n",
      "2059                                     My heart sank.    0     0      0   \n",
      "\n",
      "      Sadness  Surprise      V      A      D  \\\n",
      "2124        0         0  0.730  0.840  0.293   \n",
      "2716        0         0  0.980  0.824  0.794   \n",
      "2232        3         2  0.688  0.642  0.312   \n",
      "261         2         0  0.625  0.564  0.228   \n",
      "2059        2         0  0.520  0.288  0.164   \n",
      "\n",
      "                                         text_processed  \n",
      "2124  january mention pm boys girls crawled out of b...  \n",
      "2716  we headed north on a sunny saturday morning in...  \n",
      "2232  i looked down to find five small white arrows ...  \n",
      "261                       i have never gone back there.  \n",
      "2059                                     my heart sank.  \n",
      "                         id  \\\n",
      "1378  eng_train_track2_1379   \n",
      "839    eng_train_track2_840   \n",
      "2164  eng_train_track2_2165   \n",
      "2619  eng_train_track2_2620   \n",
      "927    eng_train_track2_928   \n",
      "\n",
      "                                                   text  Joy  Fear  Anger  \\\n",
      "1378                       Colorado, middle of nowhere.    0     1      0   \n",
      "839   This involved swimming a pretty large lake tha...    0     2      0   \n",
      "2164        It was one of my most shameful experiences.    0     1      0   \n",
      "2619  After all, I had vegetables coming out my ears...    0     0      0   \n",
      "927                         Then the screaming started.    0     3      0   \n",
      "\n",
      "      Sadness  Surprise      V      A      D  \\\n",
      "1378        0         1  0.802  0.857  0.427   \n",
      "839         0         0  0.730  0.840  0.293   \n",
      "2164        3         0  0.573  0.426  0.196   \n",
      "2619        0         0  0.500  0.500  0.500   \n",
      "927         1         2  0.743  0.760  0.361   \n",
      "\n",
      "                                         text_processed  \n",
      "1378                        colorado middle of nowhere.  \n",
      "839   this involved swimming a pretty large lake tha...  \n",
      "2164        it was one of my most shameful experiences.  \n",
      "2619  after all i had vegetables coming out my ears ...  \n",
      "927                         then the screaming started.  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# Función para calcular V, A y D\n",
    "def calculate_vad(row):\n",
    "    emotions = [\"Anger\", \"Fear\", \"Joy\", \"Sadness\", \"Surprise\"]\n",
    "    total_intensity = sum(row[emotion] for emotion in emotions)\n",
    "    if total_intensity == 0:\n",
    "        return 0.500, 0.500, 0.500  # Valores para \"No Emotion\"\n",
    "\n",
    "    V = sum(row[emotion] * vad_values2[emotion][\"V\"] for emotion in emotions) / total_intensity\n",
    "    A = sum(row[emotion] * vad_values2[emotion][\"A\"] for emotion in emotions) / total_intensity\n",
    "    D = sum(row[emotion] * vad_values2[emotion][\"D\"] for emotion in emotions) / total_intensity\n",
    "    return round(V, 3), round(A, 3), round(D, 3)\n",
    "\n",
    "# Aplicar la función para calcular V, A y D\n",
    "df[[\"V\", \"A\", \"D\"]] = df.apply(calculate_vad, axis=1, result_type=\"expand\")\n",
    "\n",
    "# Preprocesar texto\n",
    "tp = TextProcessing(lang='en')\n",
    "df[\"text_processed\"] = df[\"text\"].apply(lambda x: tp.transformer(x))\n",
    "\n",
    "# Dividir los datos en entrenamiento y validación (80-20)\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(train_df.head())\n",
    "print(val_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Anger</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Surprise</th>\n",
       "      <th>V</th>\n",
       "      <th>A</th>\n",
       "      <th>D</th>\n",
       "      <th>text_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2124</th>\n",
       "      <td>eng_train_track2_2125</td>\n",
       "      <td>26 January 2011 @ 04:45 pm Boys &amp; Girls 718 Cr...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.730</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.293</td>\n",
       "      <td>january mention pm boys girls crawled out of b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2716</th>\n",
       "      <td>eng_train_track2_2717</td>\n",
       "      <td>We headed north on a sunny Saturday morning in...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.794</td>\n",
       "      <td>we headed north on a sunny saturday morning in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2232</th>\n",
       "      <td>eng_train_track2_2233</td>\n",
       "      <td>I looked down to find five small white arrows ...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.688</td>\n",
       "      <td>0.642</td>\n",
       "      <td>0.312</td>\n",
       "      <td>i looked down to find five small white arrows ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>eng_train_track2_262</td>\n",
       "      <td>I've never gone back there.</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.228</td>\n",
       "      <td>i have never gone back there.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2059</th>\n",
       "      <td>eng_train_track2_2060</td>\n",
       "      <td>My heart sank.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0.288</td>\n",
       "      <td>0.164</td>\n",
       "      <td>my heart sank.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1638</th>\n",
       "      <td>eng_train_track2_1639</td>\n",
       "      <td>She cants her hip against my waist into my sid...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.841</td>\n",
       "      <td>0.717</td>\n",
       "      <td>she cants her hip against my waist into my sid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>eng_train_track2_1096</td>\n",
       "      <td>I then did the dishes, whitened my teeth, watc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>i then did the dishes whitened my teeth watche...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130</th>\n",
       "      <td>eng_train_track2_1131</td>\n",
       "      <td>It just kind of gradually vanished over a coup...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.562</td>\n",
       "      <td>it just kind of gradually vanished over a coup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294</th>\n",
       "      <td>eng_train_track2_1295</td>\n",
       "      <td>I didn't look out of my hands.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.730</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.293</td>\n",
       "      <td>i did not look out of my hands.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>eng_train_track2_861</td>\n",
       "      <td>`` I'm fine, '' Amanda said forcefully, shrugg...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.865</td>\n",
       "      <td>0.657</td>\n",
       "      <td>i am fine amanda said forcefully shrugging off...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2214 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id  \\\n",
       "2124  eng_train_track2_2125   \n",
       "2716  eng_train_track2_2717   \n",
       "2232  eng_train_track2_2233   \n",
       "261    eng_train_track2_262   \n",
       "2059  eng_train_track2_2060   \n",
       "...                     ...   \n",
       "1638  eng_train_track2_1639   \n",
       "1095  eng_train_track2_1096   \n",
       "1130  eng_train_track2_1131   \n",
       "1294  eng_train_track2_1295   \n",
       "860    eng_train_track2_861   \n",
       "\n",
       "                                                   text  Joy  Fear  Anger  \\\n",
       "2124  26 January 2011 @ 04:45 pm Boys & Girls 718 Cr...    0     2      0   \n",
       "2716  We headed north on a sunny Saturday morning in...    1     0      0   \n",
       "2232  I looked down to find five small white arrows ...    0     3      0   \n",
       "261                         I've never gone back there.    0     2      0   \n",
       "2059                                     My heart sank.    0     0      0   \n",
       "...                                                 ...  ...   ...    ...   \n",
       "1638  She cants her hip against my waist into my sid...    2     0      0   \n",
       "1095  I then did the dishes, whitened my teeth, watc...    0     0      0   \n",
       "1130  It just kind of gradually vanished over a coup...    0     0      0   \n",
       "1294                     I didn't look out of my hands.    0     1      0   \n",
       "860   `` I'm fine, '' Amanda said forcefully, shrugg...    0     0      1   \n",
       "\n",
       "      Sadness  Surprise      V      A      D  \\\n",
       "2124        0         0  0.730  0.840  0.293   \n",
       "2716        0         0  0.980  0.824  0.794   \n",
       "2232        3         2  0.688  0.642  0.312   \n",
       "261         2         0  0.625  0.564  0.228   \n",
       "2059        2         0  0.520  0.288  0.164   \n",
       "...       ...       ...    ...    ...    ...   \n",
       "1638        0         1  0.945  0.841  0.717   \n",
       "1095        0         0  0.500  0.500  0.500   \n",
       "1130        0         1  0.875  0.875  0.562   \n",
       "1294        0         0  0.730  0.840  0.293   \n",
       "860         0         0  0.167  0.865  0.657   \n",
       "\n",
       "                                         text_processed  \n",
       "2124  january mention pm boys girls crawled out of b...  \n",
       "2716  we headed north on a sunny saturday morning in...  \n",
       "2232  i looked down to find five small white arrows ...  \n",
       "261                       i have never gone back there.  \n",
       "2059                                     my heart sank.  \n",
       "...                                                 ...  \n",
       "1638  she cants her hip against my waist into my sid...  \n",
       "1095  i then did the dishes whitened my teeth watche...  \n",
       "1130  it just kind of gradually vanished over a coup...  \n",
       "1294                    i did not look out of my hands.  \n",
       "860   i am fine amanda said forcefully shrugging off...  \n",
       "\n",
       "[2214 rows x 11 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "s6YcPHVnWACy",
    "outputId": "eaf00319-9ce1-4fd2-bc7b-994840cdc8b4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ubuntu/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2681: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_scheduler\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import unicodedata\n",
    "from spacy_syllables import SpacySyllables\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.functional import softmax\n",
    "# Tokenización con BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "def BERT_tokenization(df, text_column):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for sent in df[text_column]:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            sent,\n",
    "            add_special_tokens=True,\n",
    "            max_length=128,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    return torch.cat(input_ids, dim=0), torch.cat(attention_masks, dim=0)\n",
    "\n",
    "train_input_ids, train_attention_masks = BERT_tokenization(train_df, \"text_processed\")\n",
    "val_input_ids, val_attention_masks = BERT_tokenization(val_df, \"text_processed\")\n",
    "\n",
    "# Convertir etiquetas\n",
    "train_labels = torch.tensor(train_df[[\"V\", \"A\", \"D\"]].values, dtype=torch.float32)\n",
    "val_labels = torch.tensor(val_df[[\"V\", \"A\", \"D\"]].values, dtype=torch.float32)\n",
    "\n",
    "# Crear datasets\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "val_dataset = TensorDataset(val_input_ids, val_attention_masks, val_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/ubuntu/.local/lib/python3.9/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 277/277 [14:15<00:00,  3.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.6113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 70/70 [01:05<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7166\n",
      "Precision (Micro): 0.7166, Precision (Macro): 0.7153\n",
      "Recall (Micro): 0.7166, Recall (Macro): 0.7009\n",
      "F1 (Micro): 0.7166, F1 (Macro): 0.7034\n",
      "\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 277/277 [14:13<00:00,  3.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.5792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 70/70 [01:05<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7184\n",
      "Precision (Micro): 0.7184, Precision (Macro): 0.7355\n",
      "Recall (Micro): 0.7184, Recall (Macro): 0.7334\n",
      "F1 (Micro): 0.7184, F1 (Macro): 0.7184\n",
      "\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 277/277 [14:14<00:00,  3.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.5644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 70/70 [01:05<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7581\n",
      "Precision (Micro): 0.7581, Precision (Macro): 0.7579\n",
      "Recall (Micro): 0.7581, Recall (Macro): 0.7626\n",
      "F1 (Micro): 0.7581, F1 (Macro): 0.7570\n",
      "\n",
      "Final Average Metrics Across Epochs:\n",
      "Accuracy: 0.7310\n",
      "Precision (Micro): 0.7310, Precision (Macro): 0.7362\n",
      "Recall (Micro): 0.7310, Recall (Macro): 0.7323\n",
      "F1 (Micro): 0.7310, F1 (Macro): 0.7262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm  # Importamos tqdm para las barras de progreso\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size)\n",
    "validation_dataloader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=batch_size)\n",
    "\n",
    "# Configurar el modelo BERT\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=3,  # Tres etiquetas: V, A, D\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# Configurar optimizador y scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "num_epochs = 3\n",
    "num_training_steps = len(train_dataloader) * num_epochs\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_metrics(predictions, labels):\n",
    "    pred_flat = np.argmax(predictions, axis=1)\n",
    "    labels_flat = np.argmax(labels, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(labels_flat, pred_flat)\n",
    "    precision_micro = precision_score(labels_flat, pred_flat, average=\"micro\")\n",
    "    precision_macro = precision_score(labels_flat, pred_flat, average=\"macro\")\n",
    "    recall_micro = recall_score(labels_flat, pred_flat, average=\"micro\")\n",
    "    recall_macro = recall_score(labels_flat, pred_flat, average=\"macro\")\n",
    "    f1_micro = f1_score(labels_flat, pred_flat, average=\"micro\")\n",
    "    f1_macro = f1_score(labels_flat, pred_flat, average=\"macro\")\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision_micro\": precision_micro,\n",
    "        \"precision_macro\": precision_macro,\n",
    "        \"recall_micro\": recall_micro,\n",
    "        \"recall_macro\": recall_macro,\n",
    "        \"f1_micro\": f1_micro,\n",
    "        \"f1_macro\": f1_macro,\n",
    "    }\n",
    "\n",
    "def train_and_evaluate():\n",
    "    metrics_per_epoch = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in tqdm(train_dataloader, desc=\"Training\"):\n",
    "            b_input_ids, b_attention_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "            model.zero_grad()\n",
    "\n",
    "            outputs = model(b_input_ids, \n",
    "                            attention_mask=b_attention_mask, \n",
    "                            labels=b_labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "         # Evaluation\n",
    "        model.eval()\n",
    "        eval_predictions = []\n",
    "        eval_labels = []\n",
    "\n",
    "        for batch in tqdm(validation_dataloader, desc=\"Validation\"):\n",
    "            b_input_ids, b_attention_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(b_input_ids, \n",
    "                                attention_mask=b_attention_mask)\n",
    "                logits = outputs.logits\n",
    "\n",
    "            eval_predictions.extend(logits.detach().cpu().numpy())\n",
    "            eval_labels.extend(b_labels.detach().cpu().numpy())\n",
    "\n",
    "        metrics = evaluate_metrics(np.array(eval_predictions), np.array(eval_labels))\n",
    "        metrics[\"train_loss\"] = avg_train_loss\n",
    "        metrics_per_epoch.append(metrics)\n",
    "        \n",
    "\n",
    "        print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        print(f\"Precision (Micro): {metrics['precision_micro']:.4f}, Precision (Macro): {metrics['precision_macro']:.4f}\")\n",
    "        print(f\"Recall (Micro): {metrics['recall_micro']:.4f}, Recall (Macro): {metrics['recall_macro']:.4f}\")\n",
    "        print(f\"F1 (Micro): {metrics['f1_micro']:.4f}, F1 (Macro): {metrics['f1_macro']:.4f}\")\n",
    "\n",
    "    # Calculate average metrics over all epochs\n",
    "    avg_metrics = {key: np.mean([epoch[key] for epoch in metrics_per_epoch]) for key in metrics_per_epoch[0].keys()}\n",
    "\n",
    "    print(\"\\nFinal Average Metrics Across Epochs:\")\n",
    "    print(f\"Accuracy: {avg_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision (Micro): {avg_metrics['precision_micro']:.4f}, Precision (Macro): {avg_metrics['precision_macro']:.4f}\")\n",
    "    print(f\"Recall (Micro): {avg_metrics['recall_micro']:.4f}, Recall (Macro): {avg_metrics['recall_macro']:.4f}\")\n",
    "    print(f\"F1 (Micro): {avg_metrics['f1_micro']:.4f}, F1 (Macro): {avg_metrics['f1_macro']:.4f}\")\n",
    "\n",
    "    return metrics_per_epoch, avg_metrics\n",
    "\n",
    "# Run the training and evaluation\n",
    "metrics_per_epoch, avg_metrics = train_and_evaluate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anger Pearson r: 0.4875\n",
    "fear Pearson r: 0.1348\n",
    "joy Pearson r: 0.4713\n",
    "sadness Pearson r: 0.5394\n",
    "surprise Pearson r: 0.197\n",
    "\n",
    "Average Pearson r: 0.366"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo y tokenizador guardados en ./modelo_entrenado_completo\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "# Directorio donde se guardará el modelo\n",
    "output_dir = \"./modelo_entrenado_completo\"\n",
    "\n",
    "# Crear el directorio si no existe\n",
    "import os\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Guardar el modelo\n",
    "model.save_pretrained(output_dir)\n",
    "\n",
    "# Guardar el tokenizador\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"Modelo y tokenizador guardados en {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATOS DE VALIDACIÓN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "# Guardar el modelo entrenado\n",
    "torch.save(model.state_dict(), 'modelo_entrenado.pth')\n",
    "print(\"Modelo guardado exitosamente.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 70/70 [01:05<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Metrics:\n",
      "Accuracy: 0.7581\n",
      "Precision (Micro): 0.7581, Precision (Macro): 0.7579\n",
      "Recall (Micro): 0.7581, Recall (Macro): 0.7626\n",
      "F1 (Micro): 0.7581, F1 (Macro): 0.7570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def validate_model():\n",
    "    model.eval()  # Cambiar el modelo al modo de evaluación\n",
    "    eval_predictions = []\n",
    "    eval_labels = []\n",
    "\n",
    "    # Proceso de validación\n",
    "    for batch in tqdm(validation_dataloader, desc=\"Validating\"):\n",
    "        b_input_ids, b_attention_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, attention_mask=b_attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "        eval_predictions.extend(logits.detach().cpu().numpy())\n",
    "        eval_labels.extend(b_labels.detach().cpu().numpy())\n",
    "\n",
    "    # Calcular métricas\n",
    "    metrics = evaluate_metrics(np.array(eval_predictions), np.array(eval_labels))\n",
    "    \n",
    "    print(\"\\nValidation Metrics:\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision (Micro): {metrics['precision_micro']:.4f}, Precision (Macro): {metrics['precision_macro']:.4f}\")\n",
    "    print(f\"Recall (Micro): {metrics['recall_micro']:.4f}, Recall (Macro): {metrics['recall_macro']:.4f}\")\n",
    "    print(f\"F1 (Micro): {metrics['f1_micro']:.4f}, F1 (Macro): {metrics['f1_macro']:.4f}\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# Ejecutar la validación\n",
    "validation_metrics = validate_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATOS PARA VALIDAR DEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eng_dev_track2_001</td>\n",
       "      <td>I have a floor shift in the morning, hopefully...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eng_dev_track2_002</td>\n",
       "      <td>What is it about this winter that is making me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eng_dev_track2_003</td>\n",
       "      <td>Longest, most awkward drive I've ever taken.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eng_dev_track2_004</td>\n",
       "      <td>I know not why, I wipe my face.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eng_dev_track2_005</td>\n",
       "      <td>And I laughed like this: garhahagar, because m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>eng_dev_track2_112</td>\n",
       "      <td>My heart sank.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>eng_dev_track2_113</td>\n",
       "      <td>I remember the sweat burning in my eyes and tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>eng_dev_track2_114</td>\n",
       "      <td>My sister was walking backwards and bumped her...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>eng_dev_track2_115</td>\n",
       "      <td>I can't breathe right, my head has been stuffe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>eng_dev_track2_116</td>\n",
       "      <td>I had to chord with my thumb.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>116 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                               text\n",
       "0    eng_dev_track2_001  I have a floor shift in the morning, hopefully...\n",
       "1    eng_dev_track2_002  What is it about this winter that is making me...\n",
       "2    eng_dev_track2_003       Longest, most awkward drive I've ever taken.\n",
       "3    eng_dev_track2_004                    I know not why, I wipe my face.\n",
       "4    eng_dev_track2_005  And I laughed like this: garhahagar, because m...\n",
       "..                  ...                                                ...\n",
       "111  eng_dev_track2_112                                     My heart sank.\n",
       "112  eng_dev_track2_113  I remember the sweat burning in my eyes and tr...\n",
       "113  eng_dev_track2_114  My sister was walking backwards and bumped her...\n",
       "114  eng_dev_track2_115  I can't breathe right, my head has been stuffe...\n",
       "115  eng_dev_track2_116                      I had to chord with my thumb.\n",
       "\n",
       "[116 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar el archivo CSV\n",
    "df1 = pd.read_csv(\"eng_dev.csv\")\n",
    "\n",
    "# Verificar la estructura del archivo\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2681: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Predicting: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:13<00:00,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo 'predicciones_decodificadas.csv' con intensidades decodificadas generado exitosamente.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>V_pred</th>\n",
       "      <th>A_pred</th>\n",
       "      <th>D_pred</th>\n",
       "      <th>Anger</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Surprise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eng_dev_track2_001</td>\n",
       "      <td>I have a floor shift in the morning, hopefully...</td>\n",
       "      <td>1.003434</td>\n",
       "      <td>0.891863</td>\n",
       "      <td>-0.302177</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eng_dev_track2_002</td>\n",
       "      <td>What is it about this winter that is making me...</td>\n",
       "      <td>0.621793</td>\n",
       "      <td>0.976994</td>\n",
       "      <td>-0.928572</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eng_dev_track2_003</td>\n",
       "      <td>Longest, most awkward drive I've ever taken.</td>\n",
       "      <td>0.839219</td>\n",
       "      <td>0.660034</td>\n",
       "      <td>-0.784878</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eng_dev_track2_004</td>\n",
       "      <td>I know not why, I wipe my face.</td>\n",
       "      <td>0.421110</td>\n",
       "      <td>-0.095308</td>\n",
       "      <td>-1.237276</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eng_dev_track2_005</td>\n",
       "      <td>And I laughed like this: garhahagar, because m...</td>\n",
       "      <td>1.796416</td>\n",
       "      <td>1.809361</td>\n",
       "      <td>0.109786</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>eng_dev_track2_112</td>\n",
       "      <td>My heart sank.</td>\n",
       "      <td>0.289251</td>\n",
       "      <td>-0.641791</td>\n",
       "      <td>-1.465758</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>eng_dev_track2_113</td>\n",
       "      <td>I remember the sweat burning in my eyes and tr...</td>\n",
       "      <td>0.788755</td>\n",
       "      <td>1.110890</td>\n",
       "      <td>-1.007916</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>eng_dev_track2_114</td>\n",
       "      <td>My sister was walking backwards and bumped her...</td>\n",
       "      <td>0.690792</td>\n",
       "      <td>1.230995</td>\n",
       "      <td>-0.613613</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>eng_dev_track2_115</td>\n",
       "      <td>I can't breathe right, my head has been stuffe...</td>\n",
       "      <td>0.520161</td>\n",
       "      <td>-0.014261</td>\n",
       "      <td>-1.404953</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>eng_dev_track2_116</td>\n",
       "      <td>I had to chord with my thumb.</td>\n",
       "      <td>0.448587</td>\n",
       "      <td>1.069883</td>\n",
       "      <td>-0.294281</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>116 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                               text  \\\n",
       "0    eng_dev_track2_001  I have a floor shift in the morning, hopefully...   \n",
       "1    eng_dev_track2_002  What is it about this winter that is making me...   \n",
       "2    eng_dev_track2_003       Longest, most awkward drive I've ever taken.   \n",
       "3    eng_dev_track2_004                    I know not why, I wipe my face.   \n",
       "4    eng_dev_track2_005  And I laughed like this: garhahagar, because m...   \n",
       "..                  ...                                                ...   \n",
       "111  eng_dev_track2_112                                     My heart sank.   \n",
       "112  eng_dev_track2_113  I remember the sweat burning in my eyes and tr...   \n",
       "113  eng_dev_track2_114  My sister was walking backwards and bumped her...   \n",
       "114  eng_dev_track2_115  I can't breathe right, my head has been stuffe...   \n",
       "115  eng_dev_track2_116                      I had to chord with my thumb.   \n",
       "\n",
       "       V_pred    A_pred    D_pred  Anger  Fear  Joy  Sadness  Surprise  \n",
       "0    1.003434  0.891863 -0.302177      0     1    0        1         1  \n",
       "1    0.621793  0.976994 -0.928572      0     1    0        1         1  \n",
       "2    0.839219  0.660034 -0.784878      0     1    0        1         1  \n",
       "3    0.421110 -0.095308 -1.237276      0     1    0        1         0  \n",
       "4    1.796416  1.809361  0.109786      0     1    1        0         1  \n",
       "..        ...       ...       ...    ...   ...  ...      ...       ...  \n",
       "111  0.289251 -0.641791 -1.465758      0     1    0        2         0  \n",
       "112  0.788755  1.110890 -1.007916      0     1    0        1         1  \n",
       "113  0.690792  1.230995 -0.613613      0     1    0        1         1  \n",
       "114  0.520161 -0.014261 -1.404953      0     1    0        1         0  \n",
       "115  0.448587  1.069883 -0.294281      0     1    0        1         1  \n",
       "\n",
       "[116 rows x 10 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Cargar los datos del archivo CSV\n",
    "df1 = pd.read_csv(\"eng_dev.csv\")\n",
    "\n",
    "# Predecir VAD usando el modelo entrenado\n",
    "def predict_vad(df, text_column):\n",
    "    model.eval()  # Cambiar el modelo a modo evaluación\n",
    "    input_ids, attention_masks = BERT_tokenization(df, text_column)  # Tokenización de los textos\n",
    "\n",
    "    dataset = TensorDataset(input_ids, attention_masks)\n",
    "    dataloader = DataLoader(dataset, sampler=SequentialSampler(dataset), batch_size=batch_size)\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Predicting\"):\n",
    "        b_input_ids, b_attention_mask = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, attention_mask=b_attention_mask)\n",
    "            logits = outputs.logits\n",
    "            predictions.extend(logits.detach().cpu().numpy())\n",
    "\n",
    "    return np.array(predictions)\n",
    "\n",
    "# Obtener predicciones\n",
    "predictions = predict_vad(df1, \"text\")\n",
    "\n",
    "# Crear un DataFrame con las predicciones de VAD\n",
    "pred_df = df1.copy()\n",
    "pred_df[[\"V_pred\", \"A_pred\", \"D_pred\"]] = predictions\n",
    "\n",
    "# Decodificar las predicciones de VAD a intensidades emocionales\n",
    "emotion_labels = list(vad_values2.keys())\n",
    "vad_matrix = np.array([[vad[\"V\"], vad[\"A\"], vad[\"D\"]] for vad in vad_values2.values()])\n",
    "\n",
    "def decode_vad_to_intensities(v, a, d):\n",
    "    input_vad = np.array([v, a, d])\n",
    "    distances = np.linalg.norm(vad_matrix - input_vad, axis=1)\n",
    "\n",
    "    # Invertir las distancias para calcular intensidades proporcionales\n",
    "    max_distance = np.max(distances)\n",
    "    inverted_distances = max_distance - distances\n",
    "    intensities = (inverted_distances / inverted_distances.sum()) * 3  # Escalar a 0-3\n",
    "    intensities = np.round(intensities).astype(int)  # Redondear a enteros\n",
    "    return pd.Series(intensities, index=emotion_labels)\n",
    "\n",
    "# Aplicar la decodificación a las predicciones\n",
    "decoded_intensities = pred_df.apply(\n",
    "    lambda row: decode_vad_to_intensities(row[\"V_pred\"], row[\"A_pred\"], row[\"D_pred\"]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Añadir las intensidades decodificadas al DataFrame\n",
    "for emotion in emotion_labels:\n",
    "    pred_df[emotion] = decoded_intensities[emotion]\n",
    "\n",
    "# Guardar el DataFrame con las predicciones y las intensidades decodificadas\n",
    "pred_df.to_csv(\"predicciones_decodificadas.csv\", index=False)\n",
    "print(\"Archivo 'predicciones_decodificadas.csv' con intensidades decodificadas generado exitosamente.\")\n",
    "\n",
    "# Visualizar las primeras filas del DataFrame resultante\n",
    "pred_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   id                                               text  \\\n",
      "0  eng_dev_track2_001  I have a floor shift in the morning, hopefully...   \n",
      "1  eng_dev_track2_002  What is it about this winter that is making me...   \n",
      "2  eng_dev_track2_003       Longest, most awkward drive I've ever taken.   \n",
      "3  eng_dev_track2_004                    I know not why, I wipe my face.   \n",
      "4  eng_dev_track2_005  And I laughed like this: garhahagar, because m...   \n",
      "\n",
      "   Anger  Fear  Joy  Sadness  Surprise  \n",
      "0      0     1    0        1         1  \n",
      "1      0     1    0        1         1  \n",
      "2      0     1    0        1         1  \n",
      "3      0     1    0        1         0  \n",
      "4      0     1    1        0         1  \n"
     ]
    }
   ],
   "source": [
    "# Lista de columnas a excluir\n",
    "columns_to_exclude = ['V_pred', 'A_pred', 'D_pred']  # Reemplaza con los nombres de las columnas que deseas excluir\n",
    "\n",
    "# Crear un nuevo DataFrame sin las columnas especificadas\n",
    "pred_df_filtered = pred_df.drop(columns=columns_to_exclude)\n",
    "\n",
    "# Verificar el resultado\n",
    "print(pred_df_filtered.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El archivo 'pred_eng.csv' se ha guardado con las columnas reorganizadas.\n"
     ]
    }
   ],
   "source": [
    "# Reorganizar las columnas\n",
    "columns_order = ['id','Anger', 'Fear', 'Joy', 'Sadness', 'Surprise']\n",
    "pred_df_filtered = pred_df_filtered[columns_order]\n",
    "\n",
    "# Guardar el DataFrame reorganizado en un nuevo archivo CSV\n",
    "pred_df_filtered.to_csv(\"pred_eng_b.csv\", index=False)\n",
    "\n",
    "# Confirmar que el archivo ha sido guardado\n",
    "print(\"El archivo 'pred_eng.csv' se ha guardado con las columnas reorganizadas.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modelo_entrenado.pth existe: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Verificar si el archivo existe en el directorio actual\n",
    "print(\"modelo_entrenado.pth existe:\", os.path.exists('modelo_entrenado.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='modelo_entrenado.pth' target='_blank'>modelo_entrenado.pth</a><br>"
      ],
      "text/plain": [
       "/home/ubuntu/melissa_old/modelo_entrenado.pth"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "\n",
    "# Crear un enlace para descargar el archivo\n",
    "FileLink('modelo_entrenado.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATOS DE TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>anger</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eng_test_track_b_00001</td>\n",
       "      <td>/ o \\ So today I went in for a new exam with D...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eng_test_track_b_00002</td>\n",
       "      <td>The image I have in my mind is this: a group o...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eng_test_track_b_00003</td>\n",
       "      <td>I slammed my fist against the door and yelled,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eng_test_track_b_00004</td>\n",
       "      <td>I could not unbend my knees.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eng_test_track_b_00005</td>\n",
       "      <td>I spent the night at the hotel, mostly hanging...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2762</th>\n",
       "      <td>eng_test_track_b_02763</td>\n",
       "      <td>Better late then never!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2763</th>\n",
       "      <td>eng_test_track_b_02764</td>\n",
       "      <td>In the last three weeks, I have started lookin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2764</th>\n",
       "      <td>eng_test_track_b_02765</td>\n",
       "      <td>But I never fell out, so it wasn't a problem.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2765</th>\n",
       "      <td>eng_test_track_b_02766</td>\n",
       "      <td>\" So I will remain positive for as long as I l...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2766</th>\n",
       "      <td>eng_test_track_b_02767</td>\n",
       "      <td>`` Bella my head is fine.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2767 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          id  \\\n",
       "0     eng_test_track_b_00001   \n",
       "1     eng_test_track_b_00002   \n",
       "2     eng_test_track_b_00003   \n",
       "3     eng_test_track_b_00004   \n",
       "4     eng_test_track_b_00005   \n",
       "...                      ...   \n",
       "2762  eng_test_track_b_02763   \n",
       "2763  eng_test_track_b_02764   \n",
       "2764  eng_test_track_b_02765   \n",
       "2765  eng_test_track_b_02766   \n",
       "2766  eng_test_track_b_02767   \n",
       "\n",
       "                                                   text  anger  fear  joy  \\\n",
       "0     / o \\ So today I went in for a new exam with D...    NaN   NaN  NaN   \n",
       "1     The image I have in my mind is this: a group o...    NaN   NaN  NaN   \n",
       "2     I slammed my fist against the door and yelled,...    NaN   NaN  NaN   \n",
       "3                          I could not unbend my knees.    NaN   NaN  NaN   \n",
       "4     I spent the night at the hotel, mostly hanging...    NaN   NaN  NaN   \n",
       "...                                                 ...    ...   ...  ...   \n",
       "2762                            Better late then never!    NaN   NaN  NaN   \n",
       "2763  In the last three weeks, I have started lookin...    NaN   NaN  NaN   \n",
       "2764      But I never fell out, so it wasn't a problem.    NaN   NaN  NaN   \n",
       "2765  \" So I will remain positive for as long as I l...    NaN   NaN  NaN   \n",
       "2766                          `` Bella my head is fine.    NaN   NaN  NaN   \n",
       "\n",
       "      sadness  surprise  \n",
       "0         NaN       NaN  \n",
       "1         NaN       NaN  \n",
       "2         NaN       NaN  \n",
       "3         NaN       NaN  \n",
       "4         NaN       NaN  \n",
       "...       ...       ...  \n",
       "2762      NaN       NaN  \n",
       "2763      NaN       NaN  \n",
       "2764      NaN       NaN  \n",
       "2765      NaN       NaN  \n",
       "2766      NaN       NaN  \n",
       "\n",
       "[2767 rows x 7 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar el archivo CSV\n",
    "df3 = pd.read_csv(\"eng.csv\")\n",
    "\n",
    "# Verificar la estructura del archivo\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2681: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Predicting: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:13<00:00,  1.10it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (116) does not match length of index (2767)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Crear un DataFrame con las predicciones de VAD\u001b[39;00m\n\u001b[1;32m     33\u001b[0m pred_df1 \u001b[38;5;241m=\u001b[39m df4\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m---> 34\u001b[0m \u001b[43mpred_df1\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mV_pred\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mA_pred\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mD_pred\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m predictions\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Decodificar las predicciones de VAD a intensidades emocionales\u001b[39;00m\n\u001b[1;32m     37\u001b[0m emotion_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(vad_values2\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/frame.py:4299\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4297\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_frame(key, value)\n\u001b[1;32m   4298\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, (Series, np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mlist\u001b[39m, Index)):\n\u001b[0;32m-> 4299\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4300\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, DataFrame):\n\u001b[1;32m   4301\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_item_frame_value(key, value)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/frame.py:4350\u001b[0m, in \u001b[0;36mDataFrame._setitem_array\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4347\u001b[0m         \u001b[38;5;28mself\u001b[39m[col] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m   4349\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m-> 4350\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iset_not_inplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4352\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(value) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4353\u001b[0m     \u001b[38;5;66;03m# list of lists\u001b[39;00m\n\u001b[1;32m   4354\u001b[0m     value \u001b[38;5;241m=\u001b[39m DataFrame(value)\u001b[38;5;241m.\u001b[39mvalues\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/frame.py:4380\u001b[0m, in \u001b[0;36mDataFrame._iset_not_inplace\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4377\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumns must be same length as key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4379\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(key):\n\u001b[0;32m-> 4380\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m igetitem(value, i)\n\u001b[1;32m   4382\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4383\u001b[0m     ilocs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_indexer_non_unique(key)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/frame.py:4311\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4308\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[1;32m   4309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4310\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[0;32m-> 4311\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/frame.py:4524\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4514\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4515\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4516\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[1;32m   4517\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4522\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[1;32m   4523\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4524\u001b[0m     value, refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sanitize_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   4527\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m   4528\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   4529\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[1;32m   4530\u001b[0m     ):\n\u001b[1;32m   4531\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[1;32m   4532\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/frame.py:5266\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   5263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m   5265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[0;32m-> 5266\u001b[0m     \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5267\u001b[0m arr \u001b[38;5;241m=\u001b[39m sanitize_array(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   5268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   5269\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(value, Index)\n\u001b[1;32m   5270\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5273\u001b[0m     \u001b[38;5;66;03m# TODO: Remove kludge in sanitize_array for string mode when enforcing\u001b[39;00m\n\u001b[1;32m   5274\u001b[0m     \u001b[38;5;66;03m# this deprecation\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/common.py:573\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[0;32m--> 573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    577\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    578\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (116) does not match length of index (2767)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Cargar los datos del archivo CSV\n",
    "df4 = pd.read_csv(\"eng.csv\")\n",
    "\n",
    "# Predecir VAD usando el modelo entrenado\n",
    "def predict_vad(df, text_column):\n",
    "    model.eval()  # Cambiar el modelo a modo evaluación\n",
    "    input_ids, attention_masks = BERT_tokenization(df, text_column)  # Tokenización de los textos\n",
    "\n",
    "    dataset = TensorDataset(input_ids, attention_masks)\n",
    "    dataloader = DataLoader(dataset, sampler=SequentialSampler(dataset), batch_size=batch_size)\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Predicting\"):\n",
    "        b_input_ids, b_attention_mask = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, attention_mask=b_attention_mask)\n",
    "            logits = outputs.logits\n",
    "            predictions.extend(logits.detach().cpu().numpy())\n",
    "\n",
    "    return np.array(predictions)\n",
    "\n",
    "# Obtener predicciones\n",
    "predictions = predict_vad(df1, \"text\")\n",
    "\n",
    "# Crear un DataFrame con las predicciones de VAD\n",
    "pred_df1 = df4.copy()\n",
    "pred_df1[[\"V_pred\", \"A_pred\", \"D_pred\"]] = predictions\n",
    "\n",
    "# Decodificar las predicciones de VAD a intensidades emocionales\n",
    "emotion_labels = list(vad_values2.keys())\n",
    "vad_matrix = np.array([[vad[\"V\"], vad[\"A\"], vad[\"D\"]] for vad in vad_values2.values()])\n",
    "\n",
    "def decode_vad_to_intensities(v, a, d):\n",
    "    input_vad = np.array([v, a, d])\n",
    "    distances = np.linalg.norm(vad_matrix - input_vad, axis=1)\n",
    "\n",
    "    # Invertir las distancias para calcular intensidades proporcionales\n",
    "    max_distance = np.max(distances)\n",
    "    inverted_distances = max_distance - distances\n",
    "    intensities = (inverted_distances / inverted_distances.sum()) * 3  # Escalar a 0-3\n",
    "    intensities = np.round(intensities).astype(int)  # Redondear a enteros\n",
    "    return pd.Series(intensities, index=emotion_labels)\n",
    "\n",
    "# Aplicar la decodificación a las predicciones\n",
    "decoded_intensities = pred_df1.apply(\n",
    "    lambda row: decode_vad_to_intensities(row[\"V_pred\"], row[\"A_pred\"], row[\"D_pred\"]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Añadir las intensidades decodificadas al DataFrame\n",
    "for emotion in emotion_labels:\n",
    "    pred_df1[emotion] = decoded_intensities[emotion]\n",
    "\n",
    "# Guardar el DataFrame con las predicciones y las intensidades decodificadas\n",
    "pred_df1.to_csv(\"predicciones_decodificadas.csv\", index=False)\n",
    "print(\"Archivo 'predicciones_decodificadas.csv' con intensidades decodificadas generado exitosamente.\")\n",
    "\n",
    "# Visualizar las primeras filas del DataFrame resultante\n",
    "pred_df1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de columnas a excluir\n",
    "columns_to_exclude = ['V_pred', 'A_pred', 'D_pred']  # Reemplaza con los nombres de las columnas que deseas excluir\n",
    "\n",
    "# Crear un nuevo DataFrame sin las columnas especificadas\n",
    "pred_df_filtered = pred_df1.drop(columns=columns_to_exclude)\n",
    "\n",
    "# Verificar el resultado\n",
    "print(pred_df_filtered.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorganizar las columnas\n",
    "columns_order = ['id','Anger', 'Fear', 'Joy', 'Sadness', 'Surprise']\n",
    "pred_df_filtered = pred_df_filtered[columns_order]\n",
    "\n",
    "# Guardar el DataFrame reorganizado en un nuevo archivo CSV\n",
    "pred_df_filtered.to_csv(\"pred_eng_b.csv\", index=False)\n",
    "\n",
    "# Confirmar que el archivo ha sido guardado\n",
    "print(\"El archivo 'pred_eng.csv' se ha guardado con las columnas reorganizadas.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
